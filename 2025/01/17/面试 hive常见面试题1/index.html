<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>面试 hive常见面试题1 | Tech智汇站</title><meta name="keywords" content="面试"><meta name="author" content="智汇君"><meta name="copyright" content="智汇君"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="面试 hive常见面试题1"><meta name="application-name" content="面试 hive常见面试题1"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="面试 hive常见面试题1"><meta property="og:url" content="http://example.com/2025/01/17/面试 hive常见面试题1/index.html"><meta property="og:site_name" content="Tech智汇站"><meta property="og:description" content="hive常见面试题1Hive有索引吗？1Hive本身不支持传统意义上的索引，但可以通过分区来实现类似的效果。分区可以按照某个字段对数据进行分组，从而提高查询效率。  在Map端和Reduce端进行join的不同场景是什么？12在Map端进行join适用于一张表非常小的情况，因为此时可以将小表完全加载"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://example.com/img/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2.jpg"><meta property="article:author" content="智汇君"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://example.com/img/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2.jpg"><meta name="description" content="hive常见面试题1Hive有索引吗？1Hive本身不支持传统意义上的索引，但可以通过分区来实现类似的效果。分区可以按照某个字段对数据进行分组，从而提高查询效率。  在Map端和Reduce端进行join的不同场景是什么？12在Map端进行join适用于一张表非常小的情况，因为此时可以将小表完全加载"><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="http://example.com/2025/01/17/%E9%9D%A2%E8%AF%95%20hive%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%981/"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@1.0.17/lib/assets/font-awesome-animation.min.css#fontawesome_animation 如果有就会加载，示例值：https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@1.0.17/lib/assets/font-awesome-animation.min.css"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/swiper/swiper.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: {"enable":true,"title":"与数百名博主无限进步","addFriendPlaceholder":"昵称（请勿包含博客等字样）：\n网站地址（要求博客地址，请勿提交个人主页）：\n头像图片url（请提供尽可能清晰的图片，我会上传到我自己的图床）：\n描述：\n站点截图（可选）：\n"},
  peoplecanvas: undefined,
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎肥来！"},
  LA51: undefined,
  greetingBox: {"enable":true,"default":"晚上好👋","list":[{"greeting":"晚安😴","startTime":0,"endTime":5},{"greeting":"早上好鸭👋, 祝你一天好心情！","startTime":6,"endTime":9},{"greeting":"上午好👋, 状态很好，鼓励一下～","startTime":10,"endTime":10},{"greeting":"11点多啦, 在坚持一下就吃饭啦～","startTime":11,"endTime":11},{"greeting":"午安👋, 宝贝","startTime":12,"endTime":14},{"greeting":"🌈充实的一天辛苦啦！","startTime":14,"endTime":18},{"greeting":"19点喽, 奖励一顿丰盛的大餐吧🍔。","startTime":19,"endTime":19},{"greeting":"晚上好👋, 在属于自己的时间好好放松😌~","startTime":20,"endTime":24}]},
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  music_page_default: "nav_music",
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: undefined,
  authorStatus: {"skills":["🤖️ 数码科技爱好者","🔍 分享与热心帮助","🏠 智能家居小能手","🔨 设计开发一条龙","🤝 专修交互与设计","🏃 脚踏实地行动派","🧱 团队小组发动机","💢 壮汉人狠话不多"]},
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":270},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    simplehomepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: 智汇君","link":"链接: ","source":"来源: Tech智汇站","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'Tech智汇站',
  title: '面试 hive常见面试题1',
  postAI: '',
  pageFillDescription: 'hive常见面试题1, Hive有索引吗？, 在Map端和Reduce端进行join的不同场景是什么？, Hive的存储格式有哪些？各自的优缺点是什么？, 如何优化Hive查询性能？, Hive中的Sort By、Order By、Cluster By、Distribute By各代表什么意思？, Hive的两种模式是什么？分别适用于什么场景？, 数仓为什么要进行分层？, 关于Hive有哪些常见的优化？, 如何处理数据倾斜问题？, Hive中排序函数的区别？, Hive如何实现分区？, Hive如何进行数据的导入和导出？, 自定义函数 UDF UDTF, 计算字符串长度, 列转行 单列, 列转行 多列, 通过自定义函数(UDF)实现在SQL中将单词首字母转换成大写, 36、【学习任务】项目任务-使用Hive加载指定格式数据, 1, 2 一个复杂的案例, 运维如何对 hive 进行调度, 使用过Hive解析JSON串吗, get_json_object 自带, json_tuple 自带, Hive解析json数组, 嵌套子查询解析json数组, explode函数, regexp_replace函数, json_tuple, 使用 lateral view 解析json数组, Hive 小文件过多怎么解决, 小文件产生原因, 小文件过多产生的影响, 怎么解决小文件过多, 使用 hive 自带的 concatenate 命令自动合并小文件, 调整参数减少Map数量, 减少Reduce的数量, 使用hadoop的archive将小文件归档, Hive 优化有哪些, Hive性能问题排查方式, 实践, join 语句会过滤 null 的值吗？, group by 分组语句会进行排序吗？, 哪条sql执行效率高呢？, Hive性能调优的方式, 1. SQL语句优化, union all, distinct, 2. 数据格式优化, 3. 小文件过多优化, 4. 并行执行优化, 5. JVM优化, 6. 推测执行优化常见面试题有索引吗本身不支持传统意义上的索引但可以通过分区来实现类似的效果分区可以按照某个字段对数据进行分组从而提高查询效率在端和端进行的不同场景是什么在端进行适用于一张表非常小的情况因为此时可以将小表完全加载到内存中进行处理在端进行是最常用的方式适用于两表都较大的情况通过框架并行处理以提高效率的存储格式有哪些各自的优缺点是什么压缩率高读写速度快适合实时分析列式存储压缩率高查询性能好但写入速度较慢简单易用但压缩率低性能较差支持切分如何优化查询性能使用合适的存储格式如或进行优化如合理使用索引分区避免全表扫描调整配置参数如设置合理的副本数和副本位置利用内置优化器和统计信息来提升查询效率中的各代表什么意思课程解释对结果集进行排序对结果集进行排序通常用于查询结果的排序对内部节点进行聚类减少跨节点的数据传输对数据进行分片均匀分布到各个节点上是对数据进行全局排序是对每个中的数据进行单独排序当的数量为时和都是全局排序的排序列可以不为中出现的指定列的排序列必须为中出现的指定列适用于小数据集性能较差适用于大数据集对大数据集进行部分排序则用对大数据集进行全局排序则用和的区别是确定数据被分配到哪个中相当于对同一个字段进行的两种模式是什么分别适用于什么场景模式仅在单个节点上运行用于开发和调试远端模式将任务提交到或其他资源管理系统上执行适用于生产环境数仓为什么要进行分层用空间换时间通过大量的预处理来提升应用系统的用户体验效率因此数据仓库会存在大量冗余的数据如果不分层的话如果源业务系统的业务规则发生变化将会影响整个数据清洗过程工作量巨大在实际业务环境中源业务系统的数据结构和业务规则经常会发生变化通过数据分层管理能够简化数据清洗的过程关于有哪些常见的优化配置默认开启开启操作重新定义小表边界如果内存空闲则可以调大对分桶表不做其他可以设置为不等值连接不适用的情况联合除之前分组排序之后在等操作后面在以及其他之前仅适用于大表小表的情况不适用于多张表或复杂排序的情况优化设置特定作业的数量默认为表示将自动决定的数量单个最大处理的字节数默认为为兆限制任何查询可能使用的的最大数量默认为数量距离最大机器数还有一定余量的原因如果集群的机器数台那么每台机器不一定都有则有一些机器单独跑那么这些机器就没有也就不会分配容器跑面试的数量为什么不是越多越好资源浪费本应分配给其他任务和节点的资源被分配给会花费过多的时间在上下文切换上而非任务处理系统需要管理更多的增加了调度和通信的开销简化排序列的表示当该属性设置为时允许在存在时在子句使用排序列编号替代字段抓取中对某些情况的查询可以不必使用计算例如将设置为即可实现不走走本地模式对于小数据集采取本地模式在单台机器上处理所有的任务即可不必分配给多台机器进行处理可以明显缩短执行时间开启本地设置的最大输入数据量设置的最大输入文件个数优化默认情况下阶段同一数据分发给一个当一个数据过大时就倾斜了并不是所有的聚合操作都需要在端完成很多聚合操作都可以现在端完成部分聚合最终再端得出最终结果开启端聚合设置在端进行聚合操作的数据条目数目有数据倾斜时进行负载均衡行列过滤列处理只查询需要的列行处理表连接时先过滤数据再进行表连接动态分区开启并行执行并行的前提是系统资源比较空闲会将一个查询转化成一个或者多个阶段这样的阶段可以是阶段抽样阶段合并阶段阶段或者其他在执行中可能需要的阶段默认一次只会执行一个阶段但是对于可以并行执行的非相互依赖的阶段可以设置并行执行同一个允许最大并行度默认为开启严格模式防止用户执行一些查询方式严格模式严格模式可以禁止的类查询对于分区表除非语句中含有分区字段过滤条件来限制范围否则不允许执行对于使用了语句的查询要求必须使用语句为了执行排序过程会将所有的结果数据分发到同一个中进行处理强制要求用户增加这个语句可以防止额外执行很长一段时间限制笛卡尔积的查询选择合适的文件格式使用列式存储格式如可以显著提高查询性能优化数据存储和加载使用数据存储可以使用压缩格式合理选择数据分隔符开启启用可以使在执行查询时一次处理一批数据而不是逐行处理从而显著提高性能执行计划分析进行选择最优的执行计划可以配置为使用执行引擎替代传统的如何处理数据倾斜问题数据倾斜问题主要有以下几种空值引发的数据倾斜不同数据类型引发的数据倾斜不可拆分大文件引发的数据倾斜数据膨胀引发的数据倾斜表连接时引发的数据倾斜确实无法减少数据量引发的数据倾斜以上倾斜问题的具体解决方案可查看中排序函数的区别略如何实现分区如何进行数据的导入和导出自定义函数计算字符串长度上传包到下再重启列转行单列列转行多列通过自定义函数实现在中将单词首字母转换成大写略学习任务项目任务使用加载指定格式数据不知道怎么做中建表加载这份数据最终可以使用查询这份数据中的各科成绩效果希望最终查询出来的效果是这样的语文数学语文任务要求针对数据源中的数据进行数据清洗清洗成需要的格式使用代码进行数据清洗针对清洗之后的数据在中创建外部表针对数据中的第二列内容需要使用数组存储任务提示思路分析考虑使用中的复杂数据类型实现注意数据中的第二列内容使用这种格式是错误的一个复杂的案例运维如何对进行调度将的定义在脚本当中使用或者进行任务的调度监控任务调度页面使用过解析串吗处理数据总体来说有两个方向的路走将以字符串的方式整个入表然后通过使用函数解析已经导入到中的数据比如使用的方法获取所需要的列名在导入之前将拆成各个字段导入表的数据是已经解析过的这将需要使用第三方的详细介绍可查看自带自带解析数组如果有一个表表中字段的内容如下百度谷歌我们想把这个字段解析出来形成如下的结构百度谷歌嵌套子查询解析数组百度谷歌执行上述语句结果报错了意思是函数不能写在别的函数内也就是这里的函数不能写在里面既然函数不能写在别的里面那我们可以用子查询方式百度谷歌执行上述语句没有报错执行结果如下百度谷歌函数函数使用解析数组表中和字段的内容如下目的把字段和字段中的解析出来下面我们就开始解析拆分字段及将数组转化成多个字符串执行上述语句结果报错意思是用的时候只支持一个字段而上述语句中有两个字段所以报错了那怎么办呢要解决这个问题还得再介绍一个语法用于和等一起使用的能将一行数据拆分成多行数据在此基础上可以对拆分的数据进行聚合首先为原始表的每行调用会把一行拆分成一行或者多行在把结果组合产生一个支持别名表的虚拟表假设我们有一张用户兴趣爱好表它有两列数据第一列是第二列是用户兴趣爱好的是一个数组存储兴趣爱好的值我们要统计所有兴趣在所有用户中出现的次数对兴趣进行解析上述执行结果按照进行分组聚合即可结果介绍完之后我们再来解决上面遇到的用的时候只支持一个字段的问题注意上述语句是三个表笛卡尔积的结果所以此方式适用于数据量不是很大的情况上述语句执行结果如下如果表中还有其他字段我们可以根据其他字段筛选出符合结果的数据总结通常和一起出现为了解决不允许在存在多个字段的问题小文件过多怎么解决小文件产生原因中的小文件肯定是向表中导入数据时产生所以先看下向中导入数据的几种方式直接向表中插入数据这种方式每次插入时都会产生一个文件多次插入少量数据就会出现多个小文件但是这种方式生产环境很少使用可以说基本没有使用的通过方式加载数据导入文件导入文件夹使用方式可以导入文件或文件夹当导入一个文件时表就有一个文件当导入文件夹时表的文件数量为文件夹下所有文件的数量通过查询方式加载数据这种方式是生产环境中常用的也是最容易产生小文件的方式导入数据时会启动任务中有多少个就输出多少个文件所以文件数量数量分区数也有很多简单任务没有只有阶段则文件数量数量分区数每执行一次时中至少产生一个文件因为导入时至少会有一个像有的业务需要每分钟就要把数据同步到中这样产生的文件就会很多小文件过多产生的影响首先对底层存储来说本身就不适合存储大量小文件小文件过多会导致元数据特别大占用太多内存严重影响的性能对来说在进行查询时每个小文件都会当成一个块启动一个任务来完成而一个任务启动和初始化的时间远远大于逻辑处理的时间就会造成很大的资源浪费而且同时可执行的数量是受限的怎么解决小文件过多使用自带的命令自动合并小文件使用方法对于非分区表对于分区表举例向表中插入数据执行以上三条语句则表下就会有三个小文件在命令行执行如下语句查看表下文件数量可以看到有三个小文件然后使用进行合并再次查看表下文件数量已合并成一个文件注意命令只支持和文件类型使用命令合并小文件时不能指定合并后的文件数量但可以多次执行该命令当多次使用后文件数量不在变化这个跟参数的设置有关可设定每个文件的最小调整参数减少数量设置输入合并小文件的相关参数执行前进行小文件合并底层是的方法此方法是在中将多个文件合成一个作为输入默认每个最大输入大小这个值决定了合并后文件的数量一个节点上的至少的大小这个值决定了多个上的文件是否需要合并一个交换机下的至少的大小这个值决定了多个交换机上的文件是否需要合并设置输出和输出进行合并的相关参数设置端输出进行合并默认为设置端输出进行合并默认为设置合并文件的大小当输出文件的平均大小小于该值时启动一个独立的任务进行文件启用压缩的查询结果输出是否进行压缩的结果输出是否使用压缩减少的数量减少的数量的个数决定了输出的文件的个数所以可以调整的个数控制表的文件数量中的分区函数正好是控制中分区的然后通过设置的数量结合分区函数让数据均衡的进入每个即可设置的数量有两种方式第一种是直接设置个数第二种是设置每个的大小会根据数据总大小猜测确定一个个数默认是设置为执行以下语句将数据均衡的分配到中解释如设置数量为则使用随机生成一个数这样数据就会随机进入中防止出现有的文件过大或过小使用的将小文件归档简称是一个高效地将小文件放入块中的文件存档工具它能够将多个小文件打包成一个文件这样在减少内存使用的同时仍然允许对文件进行透明的访问用来控制归档是否可用通知在创建归档时是否可以设置父目录控制需要归档文件的大小使用以下命令进行归档对已归档的分区恢复为原文件注意归档的分区可以查看不能必须先最后如果是新集群没有历史遗留问题的话建议使用文件格式以及启用压缩这样小文件过多可以使用自带命令快速合并优化有哪些性能问题排查方式经常使用关系型数据库的同学可能知道关系型数据库的优化的诀窍看执行计划如数据库它有多种类型的执行计划通过多种执行计划的配合使用可以看到根据统计信息推演的执行计划即推断出来的未真正运行的执行计划能够观察到从数据读取到最终呈现的主要过程和中间的量化数据可以说在开发领域掌握合适的环节选用不同的执行计划调优就不是一件难事中也有执行计划但是的执行计划都是预测的这点不像和有真实的计划可以看到每个阶段的处理数据消耗的资源和处理的时间等量化数据提供的执行计划没有这些数据这意味着虽然的使用者知道整个的执行逻辑但是各阶段耗用的资源状况和整个的执行瓶颈在哪里是不清楚的想要知道所有阶段的运行信息可以查看提供的日志查看日志的链接可以在每个作业执行后在控制台打印的信息中找到提供的执行计划目前可以查看的信息有以下几种查看执行计划的基本信息即查看执行计划的扩展信息即查看数据输入依赖的信息即查看操作相关权限的信息即查看的向量化描述信息即在查询语句的前面加上关键字是查看执行计划的基本方法用打开的执行计划包含以下两部分作业的依赖关系图即每个作业的详细信息即注使用查看执行计划是性能调优中非常重要的一种方式请务必掌握总结对语句性能问题排查的方式使用查看执行计划查看提供的日志中的执行计划详解可看我之前写的这篇文章实践不懂中的说明还没入门学会能够给我们工作中使用带来极大的便利提供了命令来展示一个查询的执行计划这个执行计划对于我们了解底层原理调优排查数据倾斜等很有帮助使用语法如下后面可以跟以下可选参数注意这几个可选参数不是每个版本都支持的加上可以输出有关计划的额外信息这通常是物理信息例如文件名这些额外信息对我们用处不大输出由优化器生成的计划从版本开始支持输出查询的抽象语法树在版本删除了存在转储可能会导致错误将在版本修复在语句中使用会产生有关计划中输入的额外信息它显示了输入的各种属性显示所有的实体需要被授权执行如果存在的查询和授权失败这对于了解系统将获得哪些锁以运行指定的查询很有用从开始支持将详细信息添加到输出中以显示为什么未对和进行矢量化从开始支持用实际的行数注释计划从开始支持在中输入以下命令得到结果请逐行看完即使看不懂也要每行都看看完以上内容有什么感受是不是感觉都看不懂不要着急下面将会详细讲解每个参数相信你学完下面的内容之后再看的查询结果将游刃有余一个查询被转换为一个由一个或多个组成的序列有向无环图这些可以是也可以是负责元数据存储的也可以是负责文件系统的操作比如移动和重命名的我们将上述结果拆分看先从最外层开始包含两个大的部分各个之间的依赖性各个的执行计划先看第一部分包含两个是根说明这是开始的依赖执行完成后执行再看第二部分里面有一个一个的执行计划分为两个部分端的执行计划树端的执行计划树这两个执行计划树里面包含这条语句的端第一个操作肯定是加载表所以就是表扫描操作常见的属性表名称表统计信息包含表中数据条数数据大小等选取操作常见的属性需要的字段名称及字段类型输出的列名称表统计信息包含表中数据条数数据大小等分组聚合操作常见的属性显示聚合函数信息聚合模式值有随机聚合就是局部聚合最终聚合分组的字段如果没有分组则没有此字段聚合之后输出列名表统计信息包含分组聚合之后的数据条数数据大小等输出到操作常见属性值为空不排序值为正序排序值为倒序排序值为排序的列为两列第一列为正序第二列为倒序过滤操作常见的属性过滤条件如语句中的则此处显示操作常见的属性方式如的条件字段完成之后输出的字段完成之后生成的数据条数大小等文件输出操作常见的属性是否压缩表的信息包含输入输出文件格式化方式序列化方式等客户端获取数据操作常见的属性值为表示不限制条数其他值为限制的条数好学到这里再翻到上面的查询结果是不是感觉基本都能看懂了语句会过滤的值吗现在我们在输入以下查询计划语句问上面这条语句会过滤为的值吗执行下面语句我们来看结果为了适应页面展示仅截取了部分输出信息从上述结果可以看到这样一行说明时会自动过滤掉关联字段为值的情况但或是不会自动过滤的大家可以自行尝试下分组语句会进行排序吗看下面这条问分组语句会进行排序吗直接来看之后结果为了适应页面展示仅截取了部分输出信息我们看里面有说明按照进行分组的再往下看还有说明是按照字段进行正序排序的哪条执行效率高呢观察两条语句这两条语句输出的结果是一样的但是哪条执行效率高呢有人说第一条执行效率高因为第二条有子查询子查询会影响性能有人说第二条执行效率高因为先过滤之后在进行时的条数减少了所以执行效率就高了到底哪条效率高呢我们直接在语句前面加上看下执行计划不就知道了嘛在第一条语句前加上得到如下结果在第二条语句前加上得到如下结果大家有什么发现除了表别名不一样其他的执行计划完全一样都是先进行条件过滤在进行条件关联说明底层会自动帮我们进行优化所以这两条语句执行效率是一样的以上仅列举了个我们生产中既熟悉又有点迷糊的例子还有很多其他的用途如查看的依赖情况排查数据倾斜调优等小伙伴们可以自行尝试性能调优的方式为什么都说性能优化这项工作是比较难的因为一项技术的优化必然是一项综合性的工作它是多门技术的结合我们如果只局限于一种技术那么肯定做不好优化的下面将从多个完全不同的角度来介绍优化的多样性我们先来一起感受下数据存储及压缩针对中表的存储格式通常有和压缩格式一般使用相比与格式表占有更少的存储因为底层使用计算架构数据流是到磁盘再到而且会有很多次所以使用数据格式和压缩策略可以降低读写还能降低网络传输量这样在一定程度上可以节省存储还能提升任务执行效率通过调参优化并行执行调节参数调节参数重用设置的参数开启模式关闭推测执行设置有效地减小数据集将大表拆分成子表结合使用外部表和分区表优化大表对大表尽量减少数据集可以通过分区表避免扫描全表或者全字段大表对小表设置自动识别小表将小表放入内存中去执行优化详细剖析可查看语句优化我们简单分析上面的语句就是将每个年龄段的最大和最小的生日获取出来放到同一张表中前后的两个语句都是对同一张表按照进行分组然后分别取最大值和最小值上面的对同一张表的相同字段进行两次分组这显然造成了极大浪费我们能不能改造下呢当然是可以的为大家介绍一个语法这个语法将前置作用就是使用一张表可以进行多次插入操作开启动态分区上面的就可以对表的字段分组一次而进行两次不同的插入操作这个例子告诉我们一定要多了解语句如果我们不知道这种语法一定不会想到这种方式的先看一个去重计数这是简单统计年龄的枚举值个数为什么不用有人说因为在数据量特别大的情况下使用第一种方式能够有效避免端的数据倾斜但事实如此吗我们先不管数据量特别大这个问题就当前的业务和环境下使用一定会比上面那种子查询的方式效率高原因有以下几点上面进行去重的字段是年龄字段要知道年龄的枚举值是非常有限的就算计算岁到岁之间的年龄的最大枚举值才如果转化成来解释的话在阶段每个会对去重由于枚举值有限因而每个得到的也有限最终得到的数据量也就是数量枚举值的个数这个数量是很小的的命令会在内存中构建一个查找去重的时间复杂度是在不同版本间变动比较大有的版本会用构建的形式去重有的版本会通过排序的方式排序最优时间复杂度无法到另外第一种方式去重会转化为两个任务会消耗更多的磁盘网络资源最新的中新增了优化通过配置即使真的出现数据倾斜也可以自动优化自动改变执行的逻辑第二种方式比第一种方式代码简洁表达的意思简单明了如果没有特殊的问题代码简洁就是优这个例子告诉我们有时候我们不要过度优化调优讲究适时调优过早进行调优有可能做的是无用功甚至产生负效应在调优上投入的工作成本和回报不成正比调优需要遵循一定的原则数据格式优化提供了多种数据存储组织格式不同格式对程序的运行效率也会有极大的影响提供的格式有和等是一个二进制对结构的平面文件在早期的平台上被广泛用于输出输出格式以及作为数据存储格式是一种列式数据存储格式可以兼容多种计算引擎如和等对多层嵌套的数据结构提供了良好的性能支持是目前生产环境中数据存储的主流选择之一优化是对的一种优化它提供了一种高效的方式来存储数据同时也能够提高的读取写入和处理数据的性能能够兼容多种计算引擎事实上在实际的生产环境中已经成为了在数据存储上的主流选择之一我们执行同样的语句及同样的数据只是数据存储格式不同得到如下执行时长数据格式时间用户等待耗时分秒分秒分秒秒分秒秒注时间表示运行程序所占用服务器资源的时间用户等待耗时记录的是用户从提交作业到返回结果期间用户等待的所有时间查询类型的数据表耗时分钟查询类型的表耗时分秒时间得以极大缩短可见不同的数据存储格式也能给性能带来极大的影响小文件过多优化小文件如果过多对来说在进行查询时每个小文件都会当成一个块启动一个任务来完成而一个任务启动和初始化的时间远远大于逻辑处理的时间就会造成很大的资源浪费而且同时可执行的数量是受限的所以我们有必要对小文件过多进行优化关于小文件过多的解决的办法我之前专门写了一篇文章讲解具体可查看并行执行优化会将一个查询转化成一个或者多个阶段这样的阶段可以是阶段抽样阶段合并阶段阶段或者执行过程中可能需要的其他阶段默认情况下一次只会执行一个阶段不过某个特定的可能包含众多的阶段而这些阶段可能并非完全互相依赖的也就是说有些阶段是可以并行执行的这样可能使得整个的执行时间缩短如果有更多的阶段可以并行执行那么可能就越快完成通过设置参数值为就可以开启并发执行在共享集群中需要注意下如果中并行阶段增多那么集群利用率就会增加打开任务并行执行同一个允许最大并行度默认为当然得是在系统资源比较空闲的时候才有优势否则没资源并行也起不来优化重用是调优参数的内容其对的性能具有非常大的影响特别是对于很难避免小文件的场景或特别多的场景这类场景大多数执行时间都很短的默认配置通常是使用派生来执行和任务的这时的启动过程可能会造成相当大的开销尤其是执行的包含有成百上千任务的情况重用可以使得实例在同一个中重新使用次的值可以在的文件中进行配置通常在之间具体多少需要根据具体业务场景测试得出我们也可以在中设置这个设置来设置我们的重用这个功能的缺点是开启重用将一直占用使用到的插槽以便进行重用直到任务完成后才能释放如果某个不平衡的中有某几个执行的时间要比其他消耗的时间多的多的话那么保留的插槽就会一直空闲着却无法被其他的使用直到所有的都结束了才会释放推测执行优化在分布式集群环境下因为程序包括本身的负载不均衡或者资源分布不均等原因会造成同一个作业的多个任务之间运行速度不一致有些任务的运行速度可能明显慢于其他任务比如一个作业的某个任务进度只有而其他所有任务已经运行完毕则这些任务会拖慢作业的整体执行进度为了避免这种情况发生采用了推测执行机制它根据一定的法则推测出拖后腿的任务并为这样的任务启动一个备份任务让该任务与原始任务同时处理同一份数据并最终选用最先成功运行完成任务的计算结果作为最终结果设置开启推测执行参数的文件中进行配置本身也提供了配置项来控制的推测执行关于调优这些推测执行变量还很难给一个具体的建议如果用户因为输入数据量很大而需要执行长时间的或者的话那么启动推测执行造成的浪费是非常巨大的最后代码优化原则理透需求原则这是优化的根本把握数据全链路原则这是优化的脉络坚持代码的简洁原则这让优化更加简单没有瓶颈时谈论优化这是自寻烦恼',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-06-17 12:05:25',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="https://npm.elemecdn.com/anzhiyu-blog-static@1.0.4/img/avatar.jpg"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><div class="back-home-button"><i class="anzhiyufont anzhiyu-icon-grip-vertical"></i><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://video-b2g.pages.dev/" title="视频解析"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/视频.png" alt="视频解析"/><span class="back-menu-item-text">视频解析</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://md5-uir.pages.dev/" title="md5值解密"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/MD5.png" alt="md5值解密"/><span class="back-menu-item-text">md5值解密</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://screencheck.pages.dev/#welcome" title="屏幕坏点检查"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/屏幕外观.png" alt="屏幕坏点检查"/><span class="back-menu-item-text">屏幕坏点检查</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://htmleditor-9lo.pages.dev/" title="在线html编辑器"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/html编辑器.png" alt="在线html编辑器"/><span class="back-menu-item-text">在线html编辑器</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">博客</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://ttyong-github-io.pages.dev/" title="旧博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/博客.png" alt="旧博客"/><span class="back-menu-item-text">旧博客</span></a></div></div></div></div><a id="site-name" href="/" accesskey="h"><div class="title">Tech智汇站</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a><div id="he-plugin-simple"></div><script>var WIDGET = {
  "CONFIG": {
    "modules": "0124",
    "background": "2",
    "tmpColor": "FFFFFF",
    "tmpSize": "16",
    "cityColor": "FFFFFF",
    "citySize": "16",
    "aqiColor": "E8D87B",
    "aqiSize": "16",
    "weatherIconSize": "24",
    "alertIconSize": "18",
    "padding": "10px 10px 10px 10px",
    "shadow": "0",
    "language": "auto",
    "borderRadius": "20",
    "fixed": "true",
    "vertical": "top",
    "horizontal": "left",
    "left": "20",
    "top": "7.1",
    "key": "df245676fb434a0691ead1c63341cd94"
  }
}
</script><link rel="stylesheet" href="https://widget.qweather.net/simple/static/css/he-simple.css?v=1.4.0"/><script src="https://widget.qweather.net/simple/static/js/he-simple.js?v=1.4.0"></script></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 友链</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><i class="anzhiyufont anzhiyu-icon-link faa-tada" style="font-size: 0.9em;"></i><span> 友人帐</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> 关于本人</span></a></li></ul></div></div></div><div id="nav-right"><div class="nav-button only-home" id="travellings_button" title="随机前往一个开往项目网站"><a class="site-page" onclick="anzhiyu.totraveling()" title="随机前往一个开往项目网站" href="javascript:void(0);" rel="external nofollow" data-pjax-state="external"><i class="anzhiyufont anzhiyu-icon-train"></i></a></div><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="/img/%E5%BE%AE%E4%BF%A1%E6%94%B6%E6%AC%BE%E7%A0%81.jpg" target="_blank"><img class="post-qr-code-img" alt="微信" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/%E5%BE%AE%E4%BF%A1%E6%94%B6%E6%AC%BE%E7%A0%81.jpg"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/%E6%94%AF%E4%BB%98%E5%AE%9D%E6%94%B6%E6%AC%BE%E7%A0%81.jpg" target="_blank"><img class="post-qr-code-img" alt="支付宝" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/%E6%94%AF%E4%BB%98%E5%AE%9D%E6%94%B6%E6%AC%BE%E7%A0%81.jpg"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/Git/" style="font-size: 1.05rem;">Git<sup>1</sup></a><a href="/tags/GitHub/" style="font-size: 1.05rem;">GitHub<sup>1</sup></a><a href="/tags/Hexo/" style="font-size: 1.05rem;">Hexo<sup>1</sup></a><a href="/tags/NodeJs/" style="font-size: 1.05rem;">NodeJs<sup>1</sup></a><a href="/tags/git/" style="font-size: 1.05rem;">git<sup>3</sup></a><a href="/tags/github/" style="font-size: 1.05rem;">github<sup>1</sup></a><a href="/tags/idea/" style="font-size: 1.05rem;">idea<sup>1</sup></a><a href="/tags/%E5%85%AC%E8%80%83%E6%9D%A8%E8%80%81%E5%B8%88/" style="font-size: 1.05rem;">公考杨老师<sup>10</sup></a><a href="/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 1.05rem;">博客<sup>1</sup></a><a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 1.05rem;">大数据<sup>26</sup></a><a href="/tags/%E5%AE%89%E7%9F%A5%E9%B1%BC%E4%B8%BB%E9%A2%98/" style="font-size: 1.05rem;">安知鱼主题<sup>1</sup></a><a href="/tags/%E5%BB%96%E9%9B%AA%E5%B3%B0/" style="font-size: 1.05rem;">廖雪峰<sup>9</sup></a><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" style="font-size: 1.05rem;">计算机网络<sup>1</sup></a><a href="/tags/%E9%9D%A2%E8%AF%95/" style="font-size: 1.05rem;">面试<sup>12</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/05/"><span class="card-archive-list-date">五月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/04/"><span class="card-archive-list-date">四月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/01/"><span class="card-archive-list-date">一月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">74</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/12/"><span class="card-archive-list-date">十二月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/07/"><span class="card-archive-list-date">七月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/01/"><span class="card-archive-list-date">一月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="post-meta-categories"><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%9D%A2%E8%AF%95/" itemprop="url">面试</a><i class="anzhiyufont anzhiyu-icon-angle-right post-meta-separator"></i><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%9D%A2%E8%AF%95/%E6%B1%82%E8%81%8C/" itemprop="url">求职</a></span></div></div><h1 class="post-title" itemprop="name headline">面试 hive常见面试题1</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="post-meta-icon anzhiyufont anzhiyu-icon-calendar-days"></i><span class="post-meta-label">发表于</span><time itemprop="dateCreated datePublished" datetime="2025-01-17T09:35:02.000Z" title="发表于 2025-01-17 17:35:02">2025-01-17</time></span></div><div class="meta-secondline"><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为重庆"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>重庆</span></div></div></div><article class="post-content" id="article-container" itemscope itemtype="http://example.com/2025/01/17/%E9%9D%A2%E8%AF%95%20hive%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%981/"><header><a class="post-meta-categories" href="/categories/%E9%9D%A2%E8%AF%95/" itemprop="url">面试</a><a class="post-meta-categories" href="/categories/%E9%9D%A2%E8%AF%95/%E6%B1%82%E8%81%8C/" itemprop="url">求职</a><h1 id="CrawlerTitle" itemprop="name headline">面试 hive常见面试题1</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">智汇君</span><time itemprop="dateCreated datePublished" datetime="2025-01-17T09:35:02.000Z" title="undefined 2025-01-17 17:35:02">2025-01-17</time></header><h1 id="hive常见面试题1"><a href="#hive常见面试题1" class="headerlink" title="hive常见面试题1"></a>hive常见面试题1</h1><h2 id="Hive有索引吗？"><a href="#Hive有索引吗？" class="headerlink" title="Hive有索引吗？"></a>Hive有索引吗？</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hive本身不支持传统意义上的索引，但可以通过分区来实现类似的效果。分区可以按照某个字段对数据进行分组，从而提高查询效率。</span><br></pre></td></tr></table></figure>

<h2 id="在Map端和Reduce端进行join的不同场景是什么？"><a href="#在Map端和Reduce端进行join的不同场景是什么？" class="headerlink" title="在Map端和Reduce端进行join的不同场景是什么？"></a>在Map端和Reduce端进行join的不同场景是什么？</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在Map端进行join适用于一张表非常小的情况，因为此时可以将小表完全加载到内存中进行处理。</span><br><span class="line">在Reduce端进行join是最常用的join方式，适用于两表都较大的情况，通过MapReduce框架并行处理以提高效率</span><br></pre></td></tr></table></figure>

<h2 id="Hive的存储格式有哪些？各自的优缺点是什么？"><a href="#Hive的存储格式有哪些？各自的优缺点是什么？" class="headerlink" title="Hive的存储格式有哪些？各自的优缺点是什么？"></a>Hive的存储格式有哪些？各自的优缺点是什么？</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ORC（OrcolLECTION Format）：压缩率高，读写速度快，适合实时分析。</span><br><span class="line">Parquet：列式存储，压缩率高，查询性能好，但写入速度较慢。</span><br><span class="line">sequenceFile：简单易用，但压缩率低，性能较差，支持切分</span><br></pre></td></tr></table></figure>

<h2 id="如何优化Hive查询性能？"><a href="#如何优化Hive查询性能？" class="headerlink" title="如何优化Hive查询性能？"></a>如何优化Hive查询性能？</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">使用合适的存储格式（如ORC或Parquet）。</span><br><span class="line">进行SQL优化，如合理使用索引（分区）、避免全表扫描。</span><br><span class="line">调整配置参数，如设置合理的副本数和副本位置。</span><br><span class="line">利用内置优化器和统计信息来提升查询效率</span><br></pre></td></tr></table></figure>

<h2 id="Hive中的Sort-By、Order-By、Cluster-By、Distribute-By各代表什么意思？"><a href="#Hive中的Sort-By、Order-By、Cluster-By、Distribute-By各代表什么意思？" class="headerlink" title="Hive中的Sort By、Order By、Cluster By、Distribute By各代表什么意思？"></a>Hive中的Sort By、Order By、Cluster By、Distribute By各代表什么意思？</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">课程解释....</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Sort By：对结果集进行排序。</span><br><span class="line">Order By：对结果集进行排序，通常用于查询结果的排序。</span><br><span class="line">Cluster By：对内部节点进行聚类，减少跨节点的数据传输。</span><br><span class="line">Distribute By：对数据进行分片，均匀分布到各个节点上。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ORDER BY是对数据进行全局排序，SORT BY是对每个Reducer中的数据进行单独排序。当Reducer的数量为1时，ORDER BY和SORT BY都是全局排序。</span><br><span class="line">ORDER BY的排序列可以不为SELECT中出现的指定列，SORT BY的排序列必须为SELECT中出现的指定列。</span><br><span class="line"></span><br><span class="line">ORDER BY适用于小数据集，性能较差;SORT BY适用于大数据集。</span><br><span class="line">对大数据集进行部分排序，则用DISTRIBUTE BY+SORT BY;对大数据集进行全局排序，则用(DISTRIBUTE BY+SORT BY|CLUSTER BY)+ORDER BY.</span><br><span class="line"></span><br><span class="line">DISTRIBUTE BY和CLUSTER BY的区别？</span><br><span class="line">DISTRIBUTE BY是确定数据被分配到哪个Reducer中，CLUSTER BY相当于对同一个字段进行DISTRIBUTE BY+SORT BY。</span><br></pre></td></tr></table></figure>



<h2 id="Hive的两种模式是什么？分别适用于什么场景？"><a href="#Hive的两种模式是什么？分别适用于什么场景？" class="headerlink" title="Hive的两种模式是什么？分别适用于什么场景？"></a>Hive的两种模式是什么？分别适用于什么场景？</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Local模式：仅在单个节点上运行，用于开发和调试。</span><br><span class="line">-远端模式：将任务提交到Yarn或其他资源管理系统上执行，适用于生产环境。</span><br></pre></td></tr></table></figure>

<h2 id="数仓为什么要进行分层？"><a href="#数仓为什么要进行分层？" class="headerlink" title="数仓为什么要进行分层？"></a>数仓为什么要进行分层？</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">用空间换时间，通过大量的预处理来提升应用系统的用户体验(效率)，因此数据仓库会存在大量冗余的数据。</span><br><span class="line">如果不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。</span><br><span class="line">(在实际业务环境中，源业务系统的数据结构和业务规则经常会发生变化。)</span><br><span class="line">通过数据分层管理能够简化数据清洗的过程。</span><br></pre></td></tr></table></figure>

<h2 id="关于Hive有哪些常见的优化？"><a href="#关于Hive有哪些常见的优化？" class="headerlink" title="关于Hive有哪些常见的优化？"></a>关于Hive有哪些常见的优化？</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Hive MapJoin</span><br><span class="line"></span><br><span class="line">配置：</span><br><span class="line">set hive.auto.convert.join = true(默认开启) -- 开启mapjoin操作</span><br><span class="line">set hive.mapjoin.smalltable.filesize=25000000; -- 重新定义小表边界，如果内存空闲，则可以调大</span><br><span class="line">set hive.optimize.bucketmapjoin=false; -- 对分桶表不做MapJoin</span><br><span class="line"></span><br><span class="line">其他:</span><br><span class="line">可以设置为不等值连接</span><br><span class="line"></span><br><span class="line">不适用的情况：</span><br><span class="line">a.&quot;联合&quot;(除UNION ALL)之前，&quot;分组排序&quot;之后</span><br><span class="line">在UNION ALL, LATERAL VIEW, GROUP BY/JOIN/SORT BY/CLUSTER BY/DISTRIBUTE BY等操作后面</span><br><span class="line">在UNION, JOIN 以及其他 MAPJOIN之前</span><br><span class="line">b.仅适用于&quot;大表+小表&quot;的情况，不适用于多张表或复杂排序的情况</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Hive Reducer优化</span><br><span class="line">设置特定MapReduce作业的reducer数量</span><br><span class="line">默认为-1，表示Hive将自动决定Reducer的数量</span><br><span class="line">set mapreduce.job.reduces=; ✔</span><br><span class="line">set mapred.reduce.tasks=; 单个Reducer最大处理的字节数</span><br><span class="line">默认为256000000，为256兆 ≈ 2Block</span><br><span class="line">set hive.exec.reducers.bytes.per.reducer=;</span><br><span class="line">* 限制任何Hive查询可能使用的reducer的最大数量</span><br><span class="line">默认为1009</span><br><span class="line">Reducer数量距离最大机器数1024还有一定余量的原因：如果集群的机器数&gt;1000台，那么每台机器不一定都有DN，则有一些机器单独跑NN，那么这些机器就没有NM，也就不会分配容器跑Reducer.</span><br><span class="line">set hive.exec.reducers.max=; 面试：Reducer的数量为什么不是越多越好？</span><br><span class="line">1.资源浪费，本应分配给其他任务和节点的资源被分配给Reducer。</span><br><span class="line">2.会花费过多的时间在上下文切换上而非任务处理。</span><br><span class="line">3.系统需要管理更多的Reducer，增加了调度和通信的开销。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">简化排序列的表示</span><br><span class="line">set hive.groupby.orderby.position.alias=true|false</span><br><span class="line">当该属性设置为true时，允许在GROUP BY存在时在ORDER BY子句使用排序列编号替代字段。</span><br><span class="line"></span><br><span class="line">Fetch抓取</span><br><span class="line">Hive中对某些情况的查询可以不必使用MapReduce计算，例如：SELECT * FROM employees;</span><br><span class="line">将hive.fetch.task.conversion设置为more即可实现不走MapReduce，走Fetch。</span><br><span class="line"></span><br><span class="line">本地模式</span><br><span class="line">对于小数据集，采取本地模式在单台机器上处理所有的任务即可，不必分配给多台机器进行处理，可以明显缩短执行时间。</span><br><span class="line">set hive.exec.mode.local.auto=true// 开启本地mr</span><br><span class="line">set hive.exec.mode.local.auto.inputbytes.max=50000000;// 设置local mr的最大输入数据量</span><br><span class="line">set hive.exec.mode.local.auto.input.files.max=10;//设置local mr的最大输入文件个数</span><br><span class="line"></span><br><span class="line">GROUP BY优化</span><br><span class="line">默认情况下，Map阶段同一Key数据分发给一个Reduce，当一个Key数据过大时就倾斜了。</span><br><span class="line">并不是所有的聚合操作都需要在Reducer端完成，很多聚合操作都可以现在Map端完成部分聚合，最终再Reducer端得出最终结果。</span><br><span class="line">set hive.map.aggr=true;//开启Map端聚合</span><br><span class="line">set hive.groupby.mapaggr.checkinterval=100000;//设置在Map端进行聚合操作的数据条目数目</span><br><span class="line">set hive.groupby.skewindata=true;//有数据倾斜时进行负载均衡</span><br><span class="line"></span><br><span class="line">行列过滤</span><br><span class="line">列处理：只查询需要的列</span><br><span class="line">行处理：表连接时，先过滤数据再进行表连接</span><br><span class="line"></span><br><span class="line">动态分区</span><br><span class="line">SET hive.exec.dynamic.partition = true;</span><br><span class="line">SET hive.exec.dynamic.partition.mode = nonstrict;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">开启并行执行(并行的前提是系统资源比较空闲)</span><br><span class="line">Hive会将一个查询转化成一个或者多个阶段，这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段...或者其他在Hive执行中可能需要的阶段。</span><br><span class="line">Hive默认一次只会执行一个阶段，但是对于可以并行执行的、非相互依赖的阶段，可以设置并行执行。</span><br><span class="line">set hive.exec.parallel=true;</span><br><span class="line">set hive.exec.parallel.thread.number=16;//同一个SQL允许最大并行度，默认为8。</span><br><span class="line"></span><br><span class="line">开启严格模式：防止用户执行一些查询方式。</span><br><span class="line">严格模式：</span><br><span class="line">set hive.mapred.mode=strict</span><br><span class="line">严格模式可以禁止的3类查询：</span><br><span class="line">1.对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。</span><br><span class="line">2.对于使用了order by语句的查询，要求必须使用limit语句。rder by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。</span><br><span class="line">3.限制笛卡尔积的查询。</span><br><span class="line"></span><br><span class="line">选择合适的文件格式</span><br><span class="line">使用列式存储格式(如Parquet,ORC)可以显著提高查询性能。</span><br><span class="line"></span><br><span class="line">优化数据存储和加载</span><br><span class="line">使用数据存储：</span><br><span class="line">可以使用Snappy压缩格式</span><br><span class="line">合理选择数据分隔符</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">开启Vetorization</span><br><span class="line">启用Vectorization可以使Hive在执行查询时一次处理一批数据，而不是逐行处理，从而显著提高性能。</span><br><span class="line">set hive.vectorized.execution.enabled=true</span><br><span class="line"></span><br><span class="line">EXPLAIN HQL</span><br><span class="line">执行计划分析，进行CBO，选择最优的执行计划。</span><br><span class="line"></span><br><span class="line">Hive可以配置为使用Tez执行引擎替代传统的MapReduce</span><br><span class="line">set hive.execution.engine=tez</span><br></pre></td></tr></table></figure>



<h2 id="如何处理数据倾斜问题？"><a href="#如何处理数据倾斜问题？" class="headerlink" title="如何处理数据倾斜问题？"></a>如何处理数据倾斜问题？</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">数据倾斜问题主要有以下几种：</span><br><span class="line"></span><br><span class="line">空值引发的数据倾斜</span><br><span class="line"></span><br><span class="line">不同数据类型引发的数据倾斜</span><br><span class="line"></span><br><span class="line">不可拆分大文件引发的数据倾斜</span><br><span class="line"></span><br><span class="line">数据膨胀引发的数据倾斜</span><br><span class="line"></span><br><span class="line">表连接时引发的数据倾斜</span><br><span class="line"></span><br><span class="line">确实无法减少数据量引发的数据倾斜</span><br><span class="line"></span><br><span class="line">以上倾斜问题的具体解决方案可查看：https://xie.infoq.cn/link?target=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fhz_6io_ZybbOlmBQE4KSBQ</span><br></pre></td></tr></table></figure>

<h2 id="Hive中排序函数的区别？"><a href="#Hive中排序函数的区别？" class="headerlink" title="Hive中排序函数的区别？"></a>Hive中排序函数的区别？</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">略</span><br></pre></td></tr></table></figure>

<h2 id="Hive如何实现分区？"><a href="#Hive如何实现分区？" class="headerlink" title="Hive如何实现分区？"></a>Hive如何实现分区？</h2><h2 id="Hive如何进行数据的导入和导出？"><a href="#Hive如何进行数据的导入和导出？" class="headerlink" title="Hive如何进行数据的导入和导出？"></a>Hive如何进行数据的导入和导出？</h2><h2 id="自定义函数-UDF-UDTF"><a href="#自定义函数-UDF-UDTF" class="headerlink" title="自定义函数 UDF UDTF"></a>自定义函数 UDF UDTF</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/ttyong/hexoBlog/master/%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B9%96Hudi/202407292154445.png" alt="image-20240729215409564"></p>
<h3 id="计算字符串长度"><a href="#计算字符串长度" class="headerlink" title="计算字符串长度"></a>计算字符串长度</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/ttyong/hexoBlog/master/%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B9%96Hudi/202407292157247.png" alt="image-20240729215702152"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/ttyong/hexoBlog/master/%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B9%96Hudi/202407292157959.png" alt="image-20240729215743506"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/ttyong/hexoBlog/master/%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B9%96Hudi/202407292158706.png" alt="image-20240729215818679"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/ttyong/hexoBlog/master/%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B9%96Hudi/202407292159742.png" alt="image-20240729215939200"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">上传jar包到hive\lib下，再重启</span><br></pre></td></tr></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/ttyong/hexoBlog/master/%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B9%96Hudi/202407292201751.png" alt="image-20240729220133515"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/ttyong/hexoBlog/master/%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B9%96Hudi/202407292201278.png" alt="image-20240729220155087"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/ttyong/hexoBlog/master/%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B9%96Hudi/202407292202544.png" alt="image-20240729220223772"></p>
<h3 id="列转行-单列"><a href="#列转行-单列" class="headerlink" title="列转行 单列"></a>列转行 单列</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/ttyong/hexoBlog/master/%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B9%96Hudi/202407292203989.png" alt="image-20240729220307194"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/ttyong/hexoBlog/master/%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B9%96Hudi/202407292203790.png" alt="image-20240729220341169"></p>
<h3 id="列转行-多列"><a href="#列转行-多列" class="headerlink" title="列转行 多列"></a>列转行 多列</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/ttyong/hexoBlog/master/%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B9%96Hudi/202407292155231.png" alt="image-20240729215508618"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/ttyong/hexoBlog/master/%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B9%96Hudi/202407292204170.png" alt="image-20240729220421704"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/ttyong/hexoBlog/master/%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B9%96Hudi/202407292204391.png" alt="image-20240729220443113"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/ttyong/hexoBlog/master/%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B9%96Hudi/202407292205144.png" alt="image-20240729220526482"></p>
<h3 id="通过自定义函数-UDF-实现在SQL中将单词首字母转换成大写"><a href="#通过自定义函数-UDF-实现在SQL中将单词首字母转换成大写" class="headerlink" title="通过自定义函数(UDF)实现在SQL中将单词首字母转换成大写"></a>通过自定义函数(UDF)实现在SQL中将单词首字母转换成大写</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">略</span><br></pre></td></tr></table></figure>

<h3 id="36、【学习任务】项目任务-使用Hive加载指定格式数据"><a href="#36、【学习任务】项目任务-使用Hive加载指定格式数据" class="headerlink" title="36、【学习任务】项目任务-使用Hive加载指定格式数据"></a>36、【学习任务】项目任务-使用Hive加载指定格式数据</h3><h4 id="1"><a href="#1" class="headerlink" title="1"></a>1</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">不知道怎么做</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">中建表，加载这份数据，最终可以使用SQL查询这份数据中的各科成绩。</span><br><span class="line"></span><br><span class="line">效果：</span><br><span class="line">希望最终查询出来的效果是这样的：</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from stu;</span><br><span class="line">stu.name       stu.scores</span><br><span class="line">zs      [&#123;&quot;subject&quot;:&quot;语文&quot;,&quot;score&quot;:81&#125;,&#123;&quot;subject&quot;:&quot;数学&quot;,&quot;score&quot;:80&#125;]</span><br><span class="line">ls      [&#123;&quot;subject&quot;:&quot;语文&quot;,&quot;score&quot;:90&#125;]</span><br><span class="line">hive (default)&gt; select scores[0].score from stu;</span><br><span class="line">score</span><br><span class="line">81</span><br><span class="line">90</span><br><span class="line"></span><br><span class="line">任务要求:</span><br><span class="line"></span><br><span class="line">1：针对数据源中的数据进行数据清洗，清洗成需要的格式，使用MapReduce代码进行数据清洗</span><br><span class="line"></span><br><span class="line">2：针对清洗之后的数据在Hive中创建外部表</span><br><span class="line"></span><br><span class="line">3：针对数据中的第二列内容需要使用数组存储</span><br><span class="line"></span><br><span class="line">任务提示、思路分析：</span><br><span class="line"></span><br><span class="line">1：考虑使用Hive中的复杂数据类型实现</span><br><span class="line"></span><br><span class="line">2：注意，数据中的第二列内容使用array&lt;string&gt;这种格式是错误的！！！</span><br></pre></td></tr></table></figure>

<h4 id="2-一个复杂的案例"><a href="#2-一个复杂的案例" class="headerlink" title="2 一个复杂的案例"></a>2 一个复杂的案例</h4><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/ttyong/hexoBlog/master/%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B9%96Hudi/202407292331499.png" alt="image-20240729233130927"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://raw.githubusercontent.com/ttyong/hexoBlog/master/%E5%B0%9A%E7%A1%85%E8%B0%B7%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B9%96Hudi/202407292334788.png" alt="image-20240729233401098"></p>
<h2 id="运维如何对-hive-进行调度"><a href="#运维如何对-hive-进行调度" class="headerlink" title="运维如何对 hive 进行调度"></a>运维如何对 hive 进行调度</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">将 hive 的 sql 定义在脚本当中；</span><br><span class="line"></span><br><span class="line">使用 azkaban 或者 oozie 进行任务的调度；</span><br><span class="line">监控任务调度页面。</span><br></pre></td></tr></table></figure>

<h2 id="使用过Hive解析JSON串吗"><a href="#使用过Hive解析JSON串吗" class="headerlink" title="使用过Hive解析JSON串吗"></a>使用过Hive解析JSON串吗</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Hive 处理 json 数据总体来说有两个方向的路走：</span><br><span class="line"></span><br><span class="line">将 json 以字符串的方式整个入 Hive 表，然后通过使用 UDF 函数解析已经导入到 hive 中的数据，比如使用LATERAL VIEW json_tuple的方法，获取所需要的列名。</span><br><span class="line"></span><br><span class="line">在导入之前将 json 拆成各个字段，导入 Hive 表的数据是已经解析过的。这将需要使用第三方的 SerDe。</span><br><span class="line"></span><br><span class="line">详细介绍可查看：https://mp.weixin.qq.com/s/awCvlb9BzCRX-Da1_l1FYg</span><br></pre></td></tr></table></figure>

<h3 id="get-json-object-自带"><a href="#get-json-object-自带" class="headerlink" title="get_json_object 自带"></a>get_json_object 自带</h3><h3 id="json-tuple-自带"><a href="#json-tuple-自带" class="headerlink" title="json_tuple 自带"></a>json_tuple 自带</h3><h3 id="Hive解析json数组"><a href="#Hive解析json数组" class="headerlink" title="Hive解析json数组"></a>Hive解析json数组</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">如果有一个hive表，表中 json_str 字段的内容如下：</span><br><span class="line"></span><br><span class="line">json_str</span><br><span class="line">[&#123;&quot;website&quot;:&quot;baidu.com&quot;,&quot;name&quot;:&quot;百度&quot;&#125;,&#123;&quot;website&quot;:&quot;google.com&quot;,&quot;name&quot;:&quot;谷歌&quot;&#125;]</span><br><span class="line">我们想把这个字段解析出来，形成如下的结构：</span><br><span class="line"></span><br><span class="line">website	    name</span><br><span class="line">baidu.com	百度</span><br><span class="line">google.com	谷歌</span><br></pre></td></tr></table></figure>



<h4 id="嵌套子查询解析json数组"><a href="#嵌套子查询解析json数组" class="headerlink" title="嵌套子查询解析json数组"></a>嵌套子查询解析json数组</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select </span><br><span class="line">json_tuple(explode(split(</span><br><span class="line">regexp_replace(regexp_replace(&#x27;[&#123;&quot;website&quot;:&quot;baidu.com&quot;,&quot;name&quot;:&quot;百度&quot;&#125;,&#123;&quot;website&quot;:&quot;google.com&quot;,&quot;name&quot;:&quot;谷歌&quot;&#125;]&#x27;, &#x27;\\[|\\]&#x27;,&#x27;&#x27;),&#x27;\\&#125;\\,\\&#123;&#x27;,&#x27;\\&#125;\\;\\&#123;&#x27;),&#x27;\\;&#x27;)) </span><br><span class="line">, &#x27;website&#x27;, &#x27;name&#x27;) ;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">执行上述语句，结果报错了：</span><br><span class="line">FAILED: SemanticException [Error 10081]: UDTF&#x27;s are not supported outside the SELECT clause, nor nested in expressions</span><br><span class="line"></span><br><span class="line">意思是UDTF函数不能写在别的函数内，也就是这里的explode函数不能写在json_tuple里面。</span><br><span class="line"></span><br><span class="line">既然explode函数不能写在别的json_tuple里面，那我们可以用子查询方式</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select json_tuple(json, &#x27;website&#x27;, &#x27;name&#x27;) </span><br><span class="line">from (</span><br><span class="line">select explode(split(regexp_replace(regexp_replace(&#x27;[&#123;&quot;website&quot;:&quot;baidu.com&quot;,&quot;name&quot;:&quot;百度&quot;&#125;,&#123;&quot;website&quot;:&quot;google.com&quot;,&quot;name&quot;:&quot;谷歌&quot;&#125;]&#x27;, &#x27;\\[|\\]&#x27;,&#x27;&#x27;),&#x27;\\&#125;\\,\\&#123;&#x27;,&#x27;\\&#125;\\;\\&#123;&#x27;),&#x27;\\;&#x27;)) </span><br><span class="line">as json) t;</span><br><span class="line">执行上述语句，没有报错，执行结果如下：</span><br><span class="line"></span><br><span class="line">www.baidu.com   百度</span><br><span class="line">google.com      谷歌</span><br></pre></td></tr></table></figure>

<h5 id="explode函数"><a href="#explode函数" class="headerlink" title="explode函数"></a>explode函数</h5><h5 id="regexp-replace函数"><a href="#regexp-replace函数" class="headerlink" title="regexp_replace函数"></a>regexp_replace函数</h5><h5 id="json-tuple"><a href="#json-tuple" class="headerlink" title="json_tuple"></a>json_tuple</h5><h4 id="使用-lateral-view-解析json数组"><a href="#使用-lateral-view-解析json数组" class="headerlink" title="使用 lateral view 解析json数组"></a>使用 lateral view 解析json数组</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive表中 goods_id 和 json_str 字段的内容如下：</span><br><span class="line"></span><br><span class="line">goods_id	json_str</span><br><span class="line">1,2,3	[&#123;&quot;source&quot;:&quot;7fresh&quot;,&quot;monthSales&quot;:4900,&quot;userCount&quot;:1900,&quot;score&quot;:&quot;9.9&quot;&#125;,&#123;&quot;source&quot;:&quot;jd&quot;,&quot;monthSales&quot;:2090,&quot;userCount&quot;:78981,&quot;score&quot;:&quot;9.8&quot;&#125;,&#123;&quot;source&quot;:&quot;jdmart&quot;,&quot;monthSales&quot;:6987,&quot;userCount&quot;:1600,&quot;score&quot;:&quot;9.0&quot;&#125;]</span><br><span class="line">目的：把 goods_id 字段和 json_str 字段中的monthSales解析出来。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">下面我们就开始解析：</span><br><span class="line"></span><br><span class="line">拆分goods_id字段及将json数组转化成多个json字符串：</span><br><span class="line">select </span><br><span class="line">explode(split(goods_id,&#x27;,&#x27;)) as good_id,</span><br><span class="line">explode(split(regexp_replace(regexp_replace(json_str , &#x27;\\[|\\]&#x27;,&#x27;&#x27;),&#x27;\\&#125;\\,\\&#123;&#x27;,&#x27;\\&#125;\\;\\&#123;&#x27;),&#x27;\\;&#x27;)) </span><br><span class="line">as sale_info </span><br><span class="line">from tableName;</span><br><span class="line">执行上述语句，结果报错：</span><br><span class="line">FAILED: SemanticException 3:0 Only a single expression in the SELECT clause is supported with UDTF&#x27;s. Error encountered near token &#x27;sale_info&#x27;</span><br><span class="line"></span><br><span class="line">意思是用UDTF的时候，SELECT 只支持一个字段。而上述语句select中有两个字段，所以报错了。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">那怎么办呢，要解决这个问题，还得再介绍一个hive语法：</span><br><span class="line">lateral view用于和split、explode等UDTF一起使用的，能将一行数据拆分成多行数据，在此基础上可以对拆分的数据进行聚合，lateral view首先为原始表的每行调用UDTF，UDTF会把一行拆分成一行或者多行，lateral view在把结果组合，产生一个支持别名表的虚拟表。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">假设我们有一张用户兴趣爱好表 hobbies_table，它有两列数据，第一列是name，第二列是用户兴趣爱好的id_list，是一个数组，存储兴趣爱好的id值：</span><br><span class="line"></span><br><span class="line">name	id_list</span><br><span class="line">zhangsan	[1,2,3]</span><br><span class="line">lisi	[3,4,5]</span><br><span class="line"></span><br><span class="line">我们要统计所有兴趣id在所有用户中出现的次数：</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">对兴趣id进行解析：</span><br><span class="line">SELECT name, hobby_id </span><br><span class="line">FROM hobbies_table </span><br><span class="line">LATERAL VIEW explode(id_list) tmp_table AS hobby_id;</span><br><span class="line"></span><br><span class="line">上述sql执行结果：</span><br><span class="line">name	hobby_id</span><br><span class="line">zhangsan	1</span><br><span class="line">zhangsan	2</span><br><span class="line">zhangsan	3</span><br><span class="line">lisi	3</span><br><span class="line">lisi	4</span><br><span class="line">lisi	5</span><br><span class="line"></span><br><span class="line">2. 按照hobby_id进行分组聚合即可：</span><br><span class="line">SELECT hobby_id ,count(name) client_num</span><br><span class="line">FROM hobbies_table </span><br><span class="line">LATERAL VIEW explode(id_list) tmp_table AS hobby_id</span><br><span class="line">GROUP BY hobby_id;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">hobby_id	client_num</span><br><span class="line">1			1</span><br><span class="line">2			1</span><br><span class="line">3			2</span><br><span class="line">4			1</span><br><span class="line">5			1</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">介绍完 lateral view 之后，我们再来解决上面遇到的用UDTF的时候，SELECT 只支持一个字段的问题：</span><br><span class="line"></span><br><span class="line">select good_id,get_json_object(sale_json,&#x27;$.monthSales&#x27;) as monthSales</span><br><span class="line">from tableName </span><br><span class="line">LATERAL VIEW explode(split(goods_id,&#x27;,&#x27;))goods as good_id </span><br><span class="line">LATERAL VIEW explode(split(regexp_replace(regexp_replace(json_str , &#x27;\\[|\\]&#x27;,&#x27;&#x27;),&#x27;\\&#125;\\,\\&#123;&#x27;,&#x27;\\&#125;\\;\\&#123;&#x27;),&#x27;\\;&#x27;)) sales as sale_json;</span><br><span class="line">注意：上述语句是三个表笛卡尔积的结果，所以此方式适用于数据量不是很大的情况。</span><br><span class="line"></span><br><span class="line">上述语句执行结果如下：</span><br><span class="line"></span><br><span class="line">goods_id	monthSales</span><br><span class="line">1	4900</span><br><span class="line">1	2090</span><br><span class="line">1	6987</span><br><span class="line">2	4900</span><br><span class="line">2	2090</span><br><span class="line">2	6987</span><br><span class="line">3	4900</span><br><span class="line">3	2090</span><br><span class="line">3	6987</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果表中还有其他字段，我们可以根据其他字段筛选出符合结果的数据。</span><br><span class="line"></span><br><span class="line">总结：lateral view通常和UDTF一起出现，为了解决UDTF不允许在select存在多个字段的问题。</span><br></pre></td></tr></table></figure>



<h2 id="Hive-小文件过多怎么解决"><a href="#Hive-小文件过多怎么解决" class="headerlink" title="Hive 小文件过多怎么解决"></a>Hive 小文件过多怎么解决</h2><h3 id="小文件产生原因"><a href="#小文件产生原因" class="headerlink" title="小文件产生原因"></a>小文件产生原因</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">hive 中的小文件肯定是向 hive 表中导入数据时产生，所以先看下向 hive 中导入数据的几种方式</span><br><span class="line"></span><br><span class="line">直接向表中插入数据</span><br><span class="line">insert into table A values (1,&#x27;zhangsan&#x27;,88),(2,&#x27;lisi&#x27;,61);</span><br><span class="line">这种方式每次插入时都会产生一个文件，多次插入少量数据就会出现多个小文件，但是这种方式生产环境很少使用，可以说基本没有使用的</span><br><span class="line"></span><br><span class="line">通过load方式加载数据</span><br><span class="line">load data local inpath &#x27;/export/score.csv&#x27; overwrite into table A  -- 导入文件</span><br><span class="line">load data local inpath &#x27;/export/score&#x27; overwrite into table A   -- 导入文件夹</span><br><span class="line">使用 load 方式可以导入文件或文件夹，当导入一个文件时，hive表就有一个文件，当导入文件夹时，hive表的文件数量为文件夹下所有文件的数量</span><br><span class="line"></span><br><span class="line">通过查询方式加载数据</span><br><span class="line">insert overwrite table A  select s_id,c_name,s_score from B;</span><br><span class="line">这种方式是生产环境中常用的，也是最容易产生小文件的方式</span><br><span class="line"></span><br><span class="line">insert 导入数据时会启动 MR 任务，MR中 reduce 有多少个就输出多少个文件</span><br><span class="line">所以， 文件数量=ReduceTask数量*分区数</span><br><span class="line">也有很多简单任务没有reduce，只有map阶段，则</span><br><span class="line">文件数量=MapTask数量*分区数</span><br><span class="line"></span><br><span class="line">每执行一次 insert 时hive中至少产生一个文件，因为 insert 导入时至少会有一个MapTask。</span><br><span class="line">像有的业务需要每10分钟就要把数据同步到 hive 中，这样产生的文件就会很多。</span><br></pre></td></tr></table></figure>

<h3 id="小文件过多产生的影响"><a href="#小文件过多产生的影响" class="headerlink" title="小文件过多产生的影响"></a>小文件过多产生的影响</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">首先对底层存储HDFS来说，HDFS本身就不适合存储大量小文件，小文件过多会导致namenode元数据特别大, 占用太多内存，严重影响HDFS的性能</span><br><span class="line"></span><br><span class="line">对 hive 来说，在进行查询时，每个小文件都会当成一个块，启动一个Map任务来完成，而一个Map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的Map数量是受限的。</span><br></pre></td></tr></table></figure>

<h3 id="怎么解决小文件过多"><a href="#怎么解决小文件过多" class="headerlink" title="怎么解决小文件过多"></a>怎么解决小文件过多</h3><h4 id="使用-hive-自带的-concatenate-命令，自动合并小文件"><a href="#使用-hive-自带的-concatenate-命令，自动合并小文件" class="headerlink" title="使用 hive 自带的 concatenate 命令，自动合并小文件"></a>使用 hive 自带的 concatenate 命令，自动合并小文件</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">使用方法：</span><br><span class="line">#对于非分区表</span><br><span class="line">alter table A concatenate;</span><br><span class="line"></span><br><span class="line">#对于分区表</span><br><span class="line">alter table B partition(day=20201224) concatenate;</span><br><span class="line"></span><br><span class="line">举例：</span><br><span class="line">#向 A 表中插入数据</span><br><span class="line">hive (default)&gt; insert into table A values (1,&#x27;aa&#x27;,67),(2,&#x27;bb&#x27;,87);</span><br><span class="line">hive (default)&gt; insert into table A values (3,&#x27;cc&#x27;,67),(4,&#x27;dd&#x27;,87);</span><br><span class="line">hive (default)&gt; insert into table A values (5,&#x27;ee&#x27;,67),(6,&#x27;ff&#x27;,87);</span><br><span class="line"></span><br><span class="line">#执行以上三条语句，则A表下就会有三个小文件,在hive命令行执行如下语句</span><br><span class="line">#查看A表下文件数量</span><br><span class="line">hive (default)&gt; dfs -ls /user/hive/warehouse/A;</span><br><span class="line">Found 3 items</span><br><span class="line">-rwxr-xr-x   3 root supergroup        378 2020-12-24 14:46 /user/hive/warehouse/A/000000_0</span><br><span class="line">-rwxr-xr-x   3 root supergroup        378 2020-12-24 14:47 /user/hive/warehouse/A/000000_0_copy_1</span><br><span class="line">-rwxr-xr-x   3 root supergroup        378 2020-12-24 14:48 /user/hive/warehouse/A/000000_0_copy_2</span><br><span class="line">#可以看到有三个小文件，然后使用 concatenate 进行合并</span><br><span class="line"></span><br><span class="line">hive (default)&gt; alter table A concatenate;</span><br><span class="line">#再次查看A表下文件数量</span><br><span class="line">hive (default)&gt; dfs -ls /user/hive/warehouse/A;</span><br><span class="line">Found 1 items</span><br><span class="line">-rwxr-xr-x   3 root supergroup        778 2020-12-24 14:59 /user/hive/warehouse/A/000000_0</span><br><span class="line">#已合并成一个文件</span><br><span class="line"></span><br><span class="line">注意： </span><br><span class="line">1、concatenate 命令只支持RCFILE和ORC文件类型。 </span><br><span class="line">2、使用concatenate命令合并小文件时不能指定合并后的文件数量，但可以多次执行该命令。 </span><br><span class="line">3、当多次使用concatenate后文件数量不在变化，这个跟参数mapreduce.input.fileinputformat.split.minsize=256mb的设置有关，可设定每个文件的最小size。</span><br></pre></td></tr></table></figure>

<h4 id="调整参数减少Map数量"><a href="#调整参数减少Map数量" class="headerlink" title="调整参数减少Map数量"></a>调整参数减少Map数量</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">设置map输入合并小文件的相关参数：</span><br><span class="line">#执行Map前进行小文件合并</span><br><span class="line">#CombineHiveInputFormat底层是 Hadoop的 CombineFileInputFormat 方法</span><br><span class="line">#此方法是在mapper中将多个文件合成一个split作为输入</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; -- 默认</span><br><span class="line">#每个Map最大输入大小(这个值决定了合并后文件的数量)</span><br><span class="line">set mapred.max.split.size=256000000;   -- 256M</span><br><span class="line">#一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)</span><br><span class="line">set mapred.min.split.size.per.node=100000000;  -- 100M</span><br><span class="line">#一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并)</span><br><span class="line">set mapred.min.split.size.per.rack=100000000;  -- 100M</span><br><span class="line"></span><br><span class="line">设置map输出和reduce输出进行合并的相关参数:</span><br><span class="line">#设置map端输出进行合并，默认为true</span><br><span class="line">set hive.merge.mapfiles = true;</span><br><span class="line">#设置reduce端输出进行合并，默认为false</span><br><span class="line">set hive.merge.mapredfiles = true;</span><br><span class="line">#设置合并文件的大小</span><br><span class="line">set hive.merge.size.per.task = 256*1000*1000;   -- 256M</span><br><span class="line">#当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge</span><br><span class="line">set hive.merge.smallfiles.avgsize=16000000;   -- 16M </span><br><span class="line"></span><br><span class="line">启用压缩</span><br><span class="line"># hive的查询结果输出是否进行压缩</span><br><span class="line">set hive.exec.compress.output=true;</span><br><span class="line"># MapReduce Job的结果输出是否使用压缩</span><br><span class="line">set mapreduce.output.fileoutputformat.compress=true;</span><br></pre></td></tr></table></figure>



<h4 id="减少Reduce的数量"><a href="#减少Reduce的数量" class="headerlink" title="减少Reduce的数量"></a>减少Reduce的数量</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">3. 减少Reduce的数量</span><br><span class="line">#reduce 的个数决定了输出的文件的个数，所以可以调整reduce的个数控制hive表的文件数量，</span><br><span class="line">#hive中的分区函数 distribute by 正好是控制MR中partition分区的，</span><br><span class="line">#然后通过设置reduce的数量，结合分区函数让数据均衡的进入每个reduce即可。</span><br><span class="line"></span><br><span class="line">#设置reduce的数量有两种方式，第一种是直接设置reduce个数</span><br><span class="line">set mapreduce.job.reduces=10;</span><br><span class="line"></span><br><span class="line">#第二种是设置每个reduce的大小，Hive会根据数据总大小猜测确定一个reduce个数</span><br><span class="line">set hive.exec.reducers.bytes.per.reducer=5120000000; -- 默认是1G，设置为5G</span><br><span class="line"></span><br><span class="line">#执行以下语句，将数据均衡的分配到reduce中</span><br><span class="line">set mapreduce.job.reduces=10;</span><br><span class="line">insert overwrite table A partition(dt)</span><br><span class="line">select * from B</span><br><span class="line">distribute by rand();</span><br><span class="line"></span><br><span class="line">解释：如设置reduce数量为10，则使用 rand()， 随机生成一个数 x % 10 ，</span><br><span class="line">这样数据就会随机进入 reduce 中，防止出现有的文件过大或过小</span><br></pre></td></tr></table></figure>

<h4 id="使用hadoop的archive将小文件归档"><a href="#使用hadoop的archive将小文件归档" class="headerlink" title="使用hadoop的archive将小文件归档"></a>使用hadoop的archive将小文件归档</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Hadoop Archive简称HAR，是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样在减少namenode内存使用的同时，仍然允许对文件进行透明的访问</span><br><span class="line"></span><br><span class="line">#用来控制归档是否可用</span><br><span class="line">set hive.archive.enabled=true;</span><br><span class="line">#通知Hive在创建归档时是否可以设置父目录</span><br><span class="line">set hive.archive.har.parentdir.settable=true;</span><br><span class="line">#控制需要归档文件的大小</span><br><span class="line">set har.partfile.size=1099511627776;</span><br><span class="line"></span><br><span class="line">#使用以下命令进行归档</span><br><span class="line">ALTER TABLE A ARCHIVE PARTITION(dt=&#x27;2020-12-24&#x27;, hr=&#x27;12&#x27;);</span><br><span class="line"></span><br><span class="line">#对已归档的分区恢复为原文件</span><br><span class="line">ALTER TABLE A UNARCHIVE PARTITION(dt=&#x27;2020-12-24&#x27;, hr=&#x27;12&#x27;);</span><br><span class="line"></span><br><span class="line">注意:  </span><br><span class="line">归档的分区可以查看不能 insert overwrite，必须先unarchive</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">最后</span><br><span class="line">如果是新集群，没有历史遗留问题的话，建议hive使用orc文件格式，以及启用 lzo 压缩。</span><br><span class="line">这样小文件过多可以使用hive自带命令concatenate快速合并</span><br></pre></td></tr></table></figure>





<h2 id="Hive-优化有哪些"><a href="#Hive-优化有哪些" class="headerlink" title="Hive 优化有哪些"></a>Hive 优化有哪些</h2><h3 id="Hive性能问题排查方式"><a href="#Hive性能问题排查方式" class="headerlink" title="Hive性能问题排查方式"></a>Hive性能问题排查方式</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">经常使用关系型数据库的同学可能知道关系型数据库的优化的诀窍-看执行计划。如Oracle数据库，它有多种类型的执行计划，通过多种执行计划的配合使用，可以看到根据统计信息推演的执行计划，即Oracle推断出来的未真正运行的执行计划；能够观察到从数据读取到最终呈现的主要过程和中间的量化数据。可以说，在Oracle开发领域，掌握合适的环节，选用不同的执行计划，SQL调优就不是一件难事。</span><br><span class="line"></span><br><span class="line">Hive中也有执行计划，但是Hive的执行计划都是预测的，这点不像Oracle和SQL Server有真实的计划，可以看到每个阶段的处理数据、消耗的资源和处理的时间等量化数据。Hive提供的执行计划没有这些数据，这意味着虽然Hive的使用者知道整个SQL的执行逻辑，但是各阶段耗用的资源状况和整个SQL的执行瓶颈在哪里是不清楚的。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">想要知道HiveSQL所有阶段的运行信息，可以查看YARN提供的日志。查看日志的链接，可以在每个作业执行后，在控制台打印的信息中找到。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Hive提供的执行计划目前可以查看的信息有以下几种：</span><br><span class="line">查看执行计划的基本信息，即explain；</span><br><span class="line">查看执行计划的扩展信息，即explain extended；</span><br><span class="line">查看SQL数据输入依赖的信息，即explain dependency；</span><br><span class="line">查看SQL操作相关权限的信息，即explain authorization；</span><br><span class="line">查看SQL的向量化描述信息，即explain vectorization。</span><br><span class="line">在查询语句的SQL前面加上关键字explain是查看执行计划的基本方法。用explain打开的执行计划包含以下两部分：</span><br><span class="line"></span><br><span class="line">作业的依赖关系图，即STAGE DEPENDENCIES；</span><br><span class="line">每个作业的详细信息，即STAGE PLANS。</span><br><span class="line"></span><br><span class="line">注：使用explain查看执行计划是Hive性能调优中非常重要的一种方式，请务必掌握！</span><br><span class="line"></span><br><span class="line">总结：Hive对SQL语句性能问题排查的方式：</span><br><span class="line">使用explain查看执行计划；</span><br><span class="line">查看YARN提供的日志。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Hive中的explain执行计划详解可看我之前写的这篇文章：https://mp.weixin.qq.com/s?__biz=Mzg2MzU2MDYzOA==&amp;mid=2247484152&amp;idx=1&amp;sn=7e48aa4a9650481f960c6cac234977a4&amp;scene=21#wechat_redirect</span><br></pre></td></tr></table></figure>

<h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">不懂hive中的explain，说明hive还没入门，学会explain，能够给我们工作中使用hive带来极大的便利！</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HIVE提供了EXPLAIN命令来展示一个查询的执行计划,这个执行计划对于我们了解底层原理，hive 调优，排查数据倾斜等很有帮助</span><br><span class="line"></span><br><span class="line">使用语法如下：</span><br><span class="line">EXPLAIN [EXTENDED|CBO|AST|DEPENDENCY|AUTHORIZATION|LOCKS|VECTORIZATION|ANALYZE] query</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">explain 后面可以跟以下可选参数，注意：这几个可选参数不是 hive 每个版本都支持的</span><br><span class="line"></span><br><span class="line">EXTENDED：加上 extended 可以输出有关计划的额外信息。这通常是物理信息，例如文件名。这些额外信息对我们用处不大</span><br><span class="line">CBO：输出由Calcite优化器生成的计划。CBO 从 hive 4.0.0 版本开始支持</span><br><span class="line">AST：输出查询的抽象语法树。AST 在hive 2.1.0 版本删除了，存在bug，转储AST可能会导致OOM错误，将在4.0.0版本修复</span><br><span class="line">DEPENDENCY：dependency在EXPLAIN语句中使用会产生有关计划中输入的额外信息。它显示了输入的各种属性</span><br><span class="line">AUTHORIZATION：显示所有的实体需要被授权执行（如果存在）的查询和授权失败</span><br><span class="line">LOCKS：这对于了解系统将获得哪些锁以运行指定的查询很有用。LOCKS 从 hive 3.2.0 开始支持</span><br><span class="line">VECTORIZATION：将详细信息添加到EXPLAIN输出中，以显示为什么未对Map和Reduce进行矢量化。从 Hive 2.3.0 开始支持</span><br><span class="line">ANALYZE：用实际的行数注释计划。从 Hive 2.2.0 开始支持</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">在 hive cli 中输入以下命令(hive 2.3.7)：</span><br><span class="line"></span><br><span class="line">explain select sum(id) from test1;</span><br><span class="line">得到结果（请逐行看完，即使看不懂也要每行都看）：</span><br><span class="line"></span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage-1 is a root stage</span><br><span class="line">  Stage-0 depends on stages: Stage-1</span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage-1</span><br><span class="line">    Map Reduce</span><br><span class="line">      Map Operator Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            alias: test1</span><br><span class="line">            Statistics: Num rows: 6 Data size: 75 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">            Select Operator</span><br><span class="line">              expressions: id (type: int)</span><br><span class="line">              outputColumnNames: id</span><br><span class="line">              Statistics: Num rows: 6 Data size: 75 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">              Group By Operator</span><br><span class="line">                aggregations: sum(id)</span><br><span class="line">                mode: hash</span><br><span class="line">                outputColumnNames: _col0</span><br><span class="line">                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">                Reduce Output Operator</span><br><span class="line">                  sort order:</span><br><span class="line">                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">                  value expressions: _col0 (type: bigint)</span><br><span class="line">      Reduce Operator Tree:</span><br><span class="line">        Group By Operator</span><br><span class="line">          aggregations: sum(VALUE._col0)</span><br><span class="line">          mode: mergepartial</span><br><span class="line">          outputColumnNames: _col0</span><br><span class="line">          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">          File Output Operator</span><br><span class="line">            compressed: false</span><br><span class="line">            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">            table:</span><br><span class="line">                input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line"></span><br><span class="line">  Stage: Stage-0</span><br><span class="line">    Fetch Operator</span><br><span class="line">      limit: -1</span><br><span class="line">      Processor Tree:</span><br><span class="line">        ListSink</span><br><span class="line"></span><br><span class="line">看完以上内容有什么感受，是不是感觉都看不懂，不要着急，下面将会详细讲解每个参数，相信你学完下面的内容之后再看 explain 的查询结果将游刃有余。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">一个HIVE查询被转换为一个由一个或多个stage组成的序列（有向无环图DAG）。这些stage可以是MapReduce stage，也可以是负责元数据存储的stage，也可以是负责文件系统的操作（比如移动和重命名）的stage。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">我们将上述结果拆分看，先从最外层开始，包含两个大的部分：</span><br><span class="line">stage dependencies： 各个stage之间的依赖性</span><br><span class="line">stage plan： 各个stage的执行计划</span><br><span class="line"></span><br><span class="line">先看第一部分 stage dependencies ，包含两个 stage，Stage-1 是根stage，说明这是开始的stage，Stage-0 依赖 Stage-1，Stage-1执行完成后执行Stage-0。</span><br><span class="line"></span><br><span class="line">再看第二部分 stage plan，里面有一个 Map Reduce，一个MR的执行计划分为两个部分：</span><br><span class="line">Map Operator Tree： MAP端的执行计划树</span><br><span class="line">Reduce Operator Tree： Reduce端的执行计划树</span><br><span class="line"></span><br><span class="line">这两个执行计划树里面包含这条sql语句的operator：</span><br><span class="line">map端第一个操作肯定是加载表，所以就是 TableScan 表扫描操作，常见的属性：</span><br><span class="line">alias： 表名称</span><br><span class="line">Statistics： 表统计信息，包含表中数据条数，数据大小等</span><br><span class="line">Select Operator： 选取操作，常见的属性 ：</span><br><span class="line">expressions：需要的字段名称及字段类型</span><br><span class="line">outputColumnNames：输出的列名称</span><br><span class="line">Statistics：表统计信息，包含表中数据条数，数据大小等</span><br><span class="line">Group By Operator：分组聚合操作，常见的属性：</span><br><span class="line">aggregations：显示聚合函数信息</span><br><span class="line">mode：聚合模式，值有 hash：随机聚合，就是hash partition；partial：局部聚合；final：最终聚合</span><br><span class="line">keys：分组的字段，如果没有分组，则没有此字段</span><br><span class="line">outputColumnNames：聚合之后输出列名</span><br><span class="line">Statistics： 表统计信息，包含分组聚合之后的数据条数，数据大小等</span><br><span class="line">Reduce Output Operator：输出到reduce操作，常见属性：</span><br><span class="line">sort order：值为空 不排序；值为 + 正序排序，值为 - 倒序排序；值为 +-  排序的列为两列，第一列为正序，第二列为倒序</span><br><span class="line">Filter Operator：过滤操作，常见的属性：</span><br><span class="line">predicate：过滤条件，如sql语句中的where id&gt;=1，则此处显示(id &gt;= 1)</span><br><span class="line">Map Join Operator：join 操作，常见的属性：</span><br><span class="line">condition map：join方式 ，如Inner Join 0 to 1 Left Outer Join0 to 2</span><br><span class="line">keys: join 的条件字段</span><br><span class="line">outputColumnNames： join 完成之后输出的字段</span><br><span class="line">Statistics： join 完成之后生成的数据条数，大小等</span><br><span class="line">File Output Operator：文件输出操作，常见的属性</span><br><span class="line">compressed：是否压缩</span><br><span class="line">table：表的信息，包含输入输出文件格式化方式，序列化方式等</span><br><span class="line">Fetch Operator 客户端获取数据操作，常见的属性：</span><br><span class="line">limit，值为 -1 表示不限制条数，其他值为限制的条数</span><br><span class="line"></span><br><span class="line">好，学到这里再翻到上面 explain 的查询结果，是不是感觉基本都能看懂了。</span><br></pre></td></tr></table></figure>

<h4 id="join-语句会过滤-null-的值吗？"><a href="#join-语句会过滤-null-的值吗？" class="headerlink" title="join 语句会过滤 null 的值吗？"></a>join 语句会过滤 null 的值吗？</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">现在，我们在hive cli 输入以下查询计划语句</span><br><span class="line"></span><br><span class="line">select a.id,b.user_name from test1 a join test2 b on a.id=b.id;</span><br><span class="line">问：上面这条 join 语句会过滤 id 为 null 的值吗</span><br><span class="line"></span><br><span class="line">执行下面语句：</span><br><span class="line">explain select a.id,b.user_name from test1 a join test2 b on a.id=b.id;</span><br><span class="line"></span><br><span class="line">我们来看结果 (为了适应页面展示，仅截取了部分输出信息)：</span><br><span class="line">TableScan</span><br><span class="line"> alias: a</span><br><span class="line"> Statistics: Num rows: 6 Data size: 75 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line"> Filter Operator</span><br><span class="line">    predicate: id is not null (type: boolean)</span><br><span class="line">    Statistics: Num rows: 6 Data size: 75 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">    Select Operator</span><br><span class="line">        expressions: id (type: int)</span><br><span class="line">        outputColumnNames: _col0</span><br><span class="line">        Statistics: Num rows: 6 Data size: 75 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">        HashTable Sink Operator</span><br><span class="line">           keys:</span><br><span class="line">             0 _col0 (type: int)</span><br><span class="line">             1 _col0 (type: int)</span><br><span class="line"> ...</span><br><span class="line"></span><br><span class="line">从上述结果可以看到 predicate: id is not null 这样一行，说明 join 时会自动过滤掉关联字段为 null 值的情况，但 left join 或 full join 是不会自动过滤的，大家可以自行尝试下。</span><br></pre></td></tr></table></figure>

<h4 id="group-by-分组语句会进行排序吗？"><a href="#group-by-分组语句会进行排序吗？" class="headerlink" title="group by 分组语句会进行排序吗？"></a>group by 分组语句会进行排序吗？</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">看下面这条sql</span><br><span class="line">select id,max(user_name) from test1 group by id;</span><br><span class="line"></span><br><span class="line">问：group by 分组语句会进行排序吗</span><br><span class="line">直接来看 explain 之后结果 (为了适应页面展示，仅截取了部分输出信息)</span><br><span class="line"> TableScan</span><br><span class="line">    alias: test1</span><br><span class="line">    Statistics: Num rows: 9 Data size: 108 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">    Select Operator</span><br><span class="line">        expressions: id (type: int), user_name (type: string)</span><br><span class="line">        outputColumnNames: id, user_name</span><br><span class="line">        Statistics: Num rows: 9 Data size: 108 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">        Group By Operator</span><br><span class="line">           aggregations: max(user_name)</span><br><span class="line">           keys: id (type: int)</span><br><span class="line">           mode: hash</span><br><span class="line">           outputColumnNames: _col0, _col1</span><br><span class="line">           Statistics: Num rows: 9 Data size: 108 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">           Reduce Output Operator</span><br><span class="line">             key expressions: _col0 (type: int)</span><br><span class="line">             sort order: +</span><br><span class="line">             Map-reduce partition columns: _col0 (type: int)</span><br><span class="line">             Statistics: Num rows: 9 Data size: 108 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">             value expressions: _col1 (type: string)</span><br><span class="line"> ...</span><br><span class="line"></span><br><span class="line">我们看 Group By Operator，里面有 keys: id (type: int) 说明按照 id 进行分组的，再往下看还有 sort order: + ，说明是按照 id 字段进行正序排序的。</span><br></pre></td></tr></table></figure>

<h4 id="哪条sql执行效率高呢？"><a href="#哪条sql执行效率高呢？" class="headerlink" title="哪条sql执行效率高呢？"></a>哪条sql执行效率高呢？</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br></pre></td><td class="code"><pre><span class="line">观察两条sql语句</span><br><span class="line">SELECT</span><br><span class="line">    a.id,</span><br><span class="line">    b.user_name</span><br><span class="line">FROM</span><br><span class="line">    test1 a</span><br><span class="line">JOIN test2 b ON a.id = b.id</span><br><span class="line">WHERE</span><br><span class="line">    a.id &gt; 2;</span><br><span class="line"></span><br><span class="line">SELECT</span><br><span class="line">    a.id,</span><br><span class="line">    b.user_name</span><br><span class="line">FROM</span><br><span class="line">    (SELECT * FROM test1 WHERE id &gt; 2) a</span><br><span class="line">JOIN test2 b ON a.id = b.id;</span><br><span class="line"></span><br><span class="line">这两条sql语句输出的结果是一样的，但是哪条sql执行效率高呢  </span><br><span class="line">有人说第一条sql执行效率高，因为第二条sql有子查询，子查询会影响性能  </span><br><span class="line">有人说第二条sql执行效率高，因为先过滤之后，在进行join时的条数减少了，所以执行效率就高了</span><br><span class="line"></span><br><span class="line">到底哪条sql效率高呢，我们直接在sql语句前面加上 explain，看下执行计划不就知道了嘛</span><br><span class="line">在第一条sql语句前加上 explain，得到如下结果</span><br><span class="line">hive (default)&gt; explain select a.id,b.user_name from test1 a join test2 b on a.id=b.id where a.id &gt;2;</span><br><span class="line">OK</span><br><span class="line">Explain</span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage-4 is a root stage</span><br><span class="line">  Stage-3 depends on stages: Stage-4</span><br><span class="line">  Stage-0 depends on stages: Stage-3</span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage-4</span><br><span class="line">    Map Reduce Local Work</span><br><span class="line">      Alias -&gt; Map Local Tables:</span><br><span class="line">        $hdt$_0:a</span><br><span class="line">          Fetch Operator</span><br><span class="line">            limit: -1</span><br><span class="line">      Alias -&gt; Map Local Operator Tree:</span><br><span class="line">        $hdt$_0:a</span><br><span class="line">          TableScan</span><br><span class="line">            alias: a</span><br><span class="line">            Statistics: Num rows: 6 Data size: 75 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">            Filter Operator</span><br><span class="line">              predicate: (id &gt; 2) (type: boolean)</span><br><span class="line">              Statistics: Num rows: 2 Data size: 25 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">              Select Operator</span><br><span class="line">                expressions: id (type: int)</span><br><span class="line">                outputColumnNames: _col0</span><br><span class="line">                Statistics: Num rows: 2 Data size: 25 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">                HashTable Sink Operator</span><br><span class="line">                  keys:</span><br><span class="line">                    0 _col0 (type: int)</span><br><span class="line">                    1 _col0 (type: int)</span><br><span class="line"></span><br><span class="line">  Stage: Stage-3</span><br><span class="line">    Map Reduce</span><br><span class="line">      Map Operator Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            alias: b</span><br><span class="line">            Statistics: Num rows: 6 Data size: 75 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">            Filter Operator</span><br><span class="line">              predicate: (id &gt; 2) (type: boolean)</span><br><span class="line">              Statistics: Num rows: 2 Data size: 25 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">              Select Operator</span><br><span class="line">                expressions: id (type: int), user_name (type: string)</span><br><span class="line">                outputColumnNames: _col0, _col1</span><br><span class="line">                Statistics: Num rows: 2 Data size: 25 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">                Map Join Operator</span><br><span class="line">                  condition map:</span><br><span class="line">                       Inner Join 0 to 1</span><br><span class="line">                  keys:</span><br><span class="line">                    0 _col0 (type: int)</span><br><span class="line">                    1 _col0 (type: int)</span><br><span class="line">                  outputColumnNames: _col0, _col2</span><br><span class="line">                  Statistics: Num rows: 2 Data size: 27 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">                  Select Operator</span><br><span class="line">                    expressions: _col0 (type: int), _col2 (type: string)</span><br><span class="line">                    outputColumnNames: _col0, _col1</span><br><span class="line">                    Statistics: Num rows: 2 Data size: 27 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">                    File Output Operator</span><br><span class="line">                      compressed: false</span><br><span class="line">                      Statistics: Num rows: 2 Data size: 27 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">                      table:</span><br><span class="line">                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line">      Local Work:</span><br><span class="line">        Map Reduce Local Work</span><br><span class="line"></span><br><span class="line">  Stage: Stage-0</span><br><span class="line">    Fetch Operator</span><br><span class="line">      limit: -1</span><br><span class="line">      Processor Tree:</span><br><span class="line">        ListSink</span><br><span class="line"></span><br><span class="line">在第二条sql语句前加上 explain，得到如下结果</span><br><span class="line">hive (default)&gt; explain select a.id,b.user_name from(select * from  test1 where id&gt;2 ) a join test2 b on a.id=b.id;</span><br><span class="line">OK</span><br><span class="line">Explain</span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage-4 is a root stage</span><br><span class="line">  Stage-3 depends on stages: Stage-4</span><br><span class="line">  Stage-0 depends on stages: Stage-3</span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage-4</span><br><span class="line">    Map Reduce Local Work</span><br><span class="line">      Alias -&gt; Map Local Tables:</span><br><span class="line">        $hdt$_0:test1</span><br><span class="line">          Fetch Operator</span><br><span class="line">            limit: -1</span><br><span class="line">      Alias -&gt; Map Local Operator Tree:</span><br><span class="line">        $hdt$_0:test1</span><br><span class="line">          TableScan</span><br><span class="line">            alias: test1</span><br><span class="line">            Statistics: Num rows: 6 Data size: 75 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">            Filter Operator</span><br><span class="line">              predicate: (id &gt; 2) (type: boolean)</span><br><span class="line">              Statistics: Num rows: 2 Data size: 25 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">              Select Operator</span><br><span class="line">                expressions: id (type: int)</span><br><span class="line">                outputColumnNames: _col0</span><br><span class="line">                Statistics: Num rows: 2 Data size: 25 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">                HashTable Sink Operator</span><br><span class="line">                  keys:</span><br><span class="line">                    0 _col0 (type: int)</span><br><span class="line">                    1 _col0 (type: int)</span><br><span class="line"></span><br><span class="line">  Stage: Stage-3</span><br><span class="line">    Map Reduce</span><br><span class="line">      Map Operator Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            alias: b</span><br><span class="line">            Statistics: Num rows: 6 Data size: 75 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">            Filter Operator</span><br><span class="line">              predicate: (id &gt; 2) (type: boolean)</span><br><span class="line">              Statistics: Num rows: 2 Data size: 25 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">              Select Operator</span><br><span class="line">                expressions: id (type: int), user_name (type: string)</span><br><span class="line">                outputColumnNames: _col0, _col1</span><br><span class="line">                Statistics: Num rows: 2 Data size: 25 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">                Map Join Operator</span><br><span class="line">                  condition map:</span><br><span class="line">                       Inner Join 0 to 1</span><br><span class="line">                  keys:</span><br><span class="line">                    0 _col0 (type: int)</span><br><span class="line">                    1 _col0 (type: int)</span><br><span class="line">                  outputColumnNames: _col0, _col2</span><br><span class="line">                  Statistics: Num rows: 2 Data size: 27 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">                  Select Operator</span><br><span class="line">                    expressions: _col0 (type: int), _col2 (type: string)</span><br><span class="line">                    outputColumnNames: _col0, _col1</span><br><span class="line">                    Statistics: Num rows: 2 Data size: 27 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">                    File Output Operator</span><br><span class="line">                      compressed: false</span><br><span class="line">                      Statistics: Num rows: 2 Data size: 27 Basic stats: COMPLETE Column stats: NONE</span><br><span class="line">                      table:</span><br><span class="line">                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line">      Local Work:</span><br><span class="line">        Map Reduce Local Work</span><br><span class="line"></span><br><span class="line">  Stage: Stage-0</span><br><span class="line">    Fetch Operator</span><br><span class="line">      limit: -1</span><br><span class="line">      Processor Tree:</span><br><span class="line">        ListSink</span><br><span class="line"></span><br><span class="line">大家有什么发现，除了表别名不一样，其他的执行计划完全一样，都是先进行 where 条件过滤，在进行 join 条件关联。说明 hive 底层会自动帮我们进行优化，所以这两条sql语句执行效率是一样的。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以上仅列举了3个我们生产中既熟悉又有点迷糊的例子，explain 还有很多其他的用途，如查看stage的依赖情况、排查数据倾斜、hive 调优等，小伙伴们可以自行尝试。</span><br></pre></td></tr></table></figure>

<h3 id="Hive性能调优的方式"><a href="#Hive性能调优的方式" class="headerlink" title="Hive性能调优的方式"></a>Hive性能调优的方式</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">为什么都说性能优化这项工作是比较难的，因为一项技术的优化，必然是一项综合性的工作，它是多门技术的结合。我们如果只局限于一种技术，那么肯定做不好优化的。</span><br><span class="line"></span><br><span class="line">下面将从多个完全不同的角度来介绍Hive优化的多样性，我们先来一起感受下。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">1. 数据存储及压缩：</span><br><span class="line">针对 hive 中表的存储格式通常有 orc 和 parquet，压缩格式一般使用 snappy。相比与 textfile 格式表，orc 占有更少的存储。因为 hive 底层使用 MR 计算架构，数据流是 hdfs 到磁盘再到 hdfs，而且会有很多次，所以使用 orc 数据格式和 snappy 压缩策略可以降低 IO 读写，还能降低网络传输量，这样在一定程度上可以节省存储，还能提升 hql 任务执行效率；</span><br><span class="line"></span><br><span class="line">2. 通过调参优化：</span><br><span class="line">并行执行，调节 parallel 参数；</span><br><span class="line"></span><br><span class="line">调节 jvm 参数，重用 jvm；</span><br><span class="line"></span><br><span class="line">设置 map、reduce 的参数；开启 strict mode 模式；</span><br><span class="line"></span><br><span class="line">关闭推测执行设置。</span><br><span class="line"></span><br><span class="line">3. 有效地减小数据集将大表拆分成子表；结合使用外部表和分区表。</span><br><span class="line">4. SQL 优化</span><br><span class="line">大表对大表：尽量减少数据集，可以通过分区表，避免扫描全表或者全字段；</span><br><span class="line"></span><br><span class="line">大表对小表：设置自动识别小表，将小表放入内存中去执行。</span><br><span class="line"></span><br><span class="line">Hive 优化详细剖析可查看：https://mp.weixin.qq.com/s/0YL0skTG9448Os3Md7CIzg</span><br></pre></td></tr></table></figure>



<h4 id="1-SQL语句优化"><a href="#1-SQL语句优化" class="headerlink" title="1. SQL语句优化"></a>1. SQL语句优化</h4><h5 id="union-all"><a href="#union-all" class="headerlink" title="union all"></a>union all</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">insert into table stu partition(tp) </span><br><span class="line">select s_age,max(s_birth) stat,&#x27;max&#x27; tp </span><br><span class="line">from stu_ori</span><br><span class="line">group by s_age</span><br><span class="line"></span><br><span class="line">union all</span><br><span class="line"></span><br><span class="line">insert into table stu partition(tp) </span><br><span class="line">select s_age,min(s_birth) stat,&#x27;min&#x27; tp </span><br><span class="line">from stu_ori</span><br><span class="line">group by s_age;</span><br><span class="line"></span><br><span class="line">我们简单分析上面的SQL语句，就是将每个年龄段的最大和最小的生日获取出来放到同一张表中，union all 前后的两个语句都是对同一张表按照s_age进行分组，然后分别取最大值和最小值。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">上面的SQL对同一张表的相同字段进行两次分组，这显然造成了极大浪费，我们能不能改造下呢，当然是可以的，为大家介绍一个语法： from ... insert into ... ，这个语法将from前置，作用就是使用一张表，可以进行多次插入操作：</span><br><span class="line"></span><br><span class="line">--开启动态分区 </span><br><span class="line">set hive.exec.dynamic.partition=true; </span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict; </span><br><span class="line"></span><br><span class="line">from stu_ori </span><br><span class="line"></span><br><span class="line">insert into table stu partition(tp) </span><br><span class="line">select s_age,max(s_birth) stat,&#x27;max&#x27; tp </span><br><span class="line">group by s_age</span><br><span class="line"></span><br><span class="line">insert into table stu partition(tp) </span><br><span class="line">select s_age,min(s_birth) stat,&#x27;min&#x27; tp </span><br><span class="line">group by s_age;</span><br><span class="line"></span><br><span class="line">上面的SQL就可以对stu_ori表的s_age字段分组一次而进行两次不同的插入操作。</span><br><span class="line"></span><br><span class="line">这个例子告诉我们一定要多了解SQL语句，如果我们不知道这种语法，一定不会想到这种方式的。</span><br></pre></td></tr></table></figure>



<h5 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">先看一个SQL，去重计数：</span><br><span class="line">select count(1) </span><br><span class="line">from( </span><br><span class="line">  select s_age </span><br><span class="line">  from stu </span><br><span class="line">  group by s_age </span><br><span class="line">) b;</span><br><span class="line"></span><br><span class="line">这是简单统计年龄的枚举值个数，为什么不用distinct？</span><br><span class="line">select count(distinct s_age) </span><br><span class="line">from stu;</span><br><span class="line"></span><br><span class="line">有人说因为在数据量特别大的情况下使用第一种方式(group by)能够有效避免Reduce端的数据倾斜，但事实如此吗？</span><br><span class="line">我们先不管数据量特别大这个问题，就当前的业务和环境下使用distinct一定会比上面那种子查询的方式效率高。原因有以下几点：</span><br><span class="line">上面进行去重的字段是年龄字段，要知道年龄的枚举值是非常有限的，就算计算1岁到100岁之间的年龄，s_age的最大枚举值才100，如果转化成MapReduce来解释的话，在Map阶段，每个Map会对s_age去重。由于s_age枚举值有限，因而每个Map得到的s_age也有限，最终得到reduce的数据量也就是map数量*s_age枚举值的个数。这个数量是很小的。</span><br><span class="line"></span><br><span class="line">distinct的命令会在内存中构建一个hashtable，查找去重的时间复杂度是O(1)；group by在不同版本间变动比较大，有的版本会用构建hashtable的形式去重，有的版本会通过排序的方式， 排序最优时间复杂度无法到O(1)。另外，第一种方式(group by)去重会转化为两个任务，会消耗更多的磁盘网络I/O资源。</span><br><span class="line"></span><br><span class="line">最新的Hive 3.0中新增了 count(distinct) 优化，通过配置 hive.optimize.countdistinct，即使真的出现数据倾斜也可以自动优化，自动改变SQL执行的逻辑。</span><br><span class="line"></span><br><span class="line">第二种方式(distinct)比第一种方式(group by)代码简洁，表达的意思简单明了，如果没有特殊的问题，代码简洁就是优！</span><br><span class="line"></span><br><span class="line">这个例子告诉我们，有时候我们不要过度优化，调优讲究适时调优，过早进行调优有可能做的是无用功甚至产生负效应，在调优上投入的工作成本和回报不成正比。调优需要遵循一定的原则。</span><br></pre></td></tr></table></figure>

<h4 id="2-数据格式优化"><a href="#2-数据格式优化" class="headerlink" title="2. 数据格式优化"></a>2. 数据格式优化</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Hive提供了多种数据存储组织格式，不同格式对程序的运行效率也会有极大的影响。</span><br><span class="line">Hive提供的格式有TEXT、SequenceFile、RCFile、ORC和Parquet等。</span><br><span class="line">SequenceFile是一个二进制key/value对结构的平面文件，在早期的Hadoop平台上被广泛用于MapReduce输出/输出格式，以及作为数据存储格式。</span><br><span class="line">Parquet是一种列式数据存储格式，可以兼容多种计算引擎，如MapRedcue和Spark等，对多层嵌套的数据结构提供了良好的性能支持，是目前Hive生产环境中数据存储的主流选择之一。</span><br><span class="line">ORC优化是对RCFile的一种优化，它提供了一种高效的方式来存储Hive数据，同时也能够提高Hive的读取、写入和处理数据的性能，能够兼容多种计算引擎。事实上，在实际的生产环境中，ORC已经成为了Hive在数据存储上的主流选择之一。</span><br><span class="line"></span><br><span class="line">我们执行同样的SQL语句及同样的数据，只是数据存储格式不同，得到如下执行时长：</span><br><span class="line">数据格式		CPU时间	用户等待耗时</span><br><span class="line">TextFile		33分		171秒</span><br><span class="line">SequenceFile	38分		162秒</span><br><span class="line">Parquet			2分22秒	50秒</span><br><span class="line">ORC				1分52秒	56秒</span><br><span class="line"></span><br><span class="line">注：CPU时间：表示运行程序所占用服务器CPU资源的时间。</span><br><span class="line">用户等待耗时：记录的是用户从提交作业到返回结果期间用户等待的所有时间。</span><br><span class="line"></span><br><span class="line">查询TextFile类型的数据表耗时33分钟， 查询ORC类型的表耗时1分52秒，时间得以极大缩短，可见不同的数据存储格式也能给HiveSQL性能带来极大的影响。</span><br></pre></td></tr></table></figure>



<h4 id="3-小文件过多优化"><a href="#3-小文件过多优化" class="headerlink" title="3. 小文件过多优化"></a>3. 小文件过多优化</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">小文件如果过多，对 hive 来说，在进行查询时，每个小文件都会当成一个块，启动一个Map任务来完成，而一个Map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的Map数量是受限的。</span><br><span class="line"></span><br><span class="line">所以我们有必要对小文件过多进行优化，关于小文件过多的解决的办法，我之前专门写了一篇文章讲解，具体可查看：https://mp.weixin.qq.com/s?__biz=Mzg2MzU2MDYzOA==&amp;mid=2247483683&amp;idx=1&amp;sn=14b25010032bdf0d375080e48de36d7f&amp;scene=21#wechat_redirect</span><br></pre></td></tr></table></figure>

<h4 id="4-并行执行优化"><a href="#4-并行执行优化" class="headerlink" title="4. 并行执行优化"></a>4. 并行执行优化</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。如果有更多的阶段可以并行执行，那么job可能就越快完成。</span><br><span class="line"></span><br><span class="line">通过设置参数hive.exec.parallel值为true，就可以开启并发执行。在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。</span><br><span class="line">set hive.exec.parallel=true; //打开任务并行执行</span><br><span class="line">set hive.exec.parallel.thread.number=16; //同一个sql允许最大并行度，默认为8。</span><br><span class="line"></span><br><span class="line">当然得是在系统资源比较空闲的时候才有优势，否则没资源，并行也起不来。</span><br></pre></td></tr></table></figure>



<h4 id="5-JVM优化"><a href="#5-JVM优化" class="headerlink" title="5. JVM优化"></a>5. JVM优化</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。</span><br><span class="line"></span><br><span class="line">Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.jvm.numtasks&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;10&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;How many tasks to run per jvm. If set to -1, there is</span><br><span class="line">  no limit. </span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">我们也可以在hive中设置</span><br><span class="line"></span><br><span class="line">set  mapred.job.reuse.jvm.num.tasks=10; //这个设置来设置我们的jvm重用</span><br><span class="line">这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。</span><br></pre></td></tr></table></figure>



<h4 id="6-推测执行优化"><a href="#6-推测执行优化" class="headerlink" title="6. 推测执行优化"></a>6. 推测执行优化</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">在分布式集群环境下，因为程序bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。</span><br><span class="line"></span><br><span class="line">设置开启推测执行参数：Hadoop的mapred-site.xml文件中进行配置：</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.speculative&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;If true, then multiple instances of some map tasks </span><br><span class="line">               may be executed in parallel.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.speculative&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;If true, then multiple instances of some reduce tasks </span><br><span class="line">               may be executed in parallel.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">hive本身也提供了配置项来控制reduce-side的推测执行:</span><br><span class="line"></span><br><span class="line">set hive.mapred.reduce.tasks.speculative.execution=true</span><br><span class="line">关于调优这些推测执行变量，还很难给一个具体的建议。如果用户因为输入数据量很大而需要执行长时间的map或者reduce task的话，那么启动推测执行造成的浪费是非常巨大的。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">最后</span><br><span class="line">代码优化原则：</span><br><span class="line"></span><br><span class="line">理透需求原则，这是优化的根本；</span><br><span class="line">把握数据全链路原则，这是优化的脉络；</span><br><span class="line">坚持代码的简洁原则，这让优化更加简单；</span><br><span class="line">没有瓶颈时谈论优化，这是自寻烦恼</span><br></pre></td></tr></table></figure>

</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">智汇君</div><div class="post-copyright__author_desc">路漫漫其修远兮，吾将上下而求索！</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="http://example.com/2025/01/17/%E9%9D%A2%E8%AF%95%20hive%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%981/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('http://example.com/2025/01/17/面试 hive常见面试题1/')">面试 hive常见面试题1</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="/img/%E5%BE%AE%E4%BF%A1%E6%94%B6%E6%AC%BE%E7%A0%81.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/%E5%BE%AE%E4%BF%A1%E6%94%B6%E6%AC%BE%E7%A0%81.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/%E6%94%AF%E4%BB%98%E5%AE%9D%E6%94%B6%E6%AC%BE%E7%A0%81.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/%E6%94%AF%E4%BB%98%E5%AE%9D%E6%94%B6%E6%AC%BE%E7%A0%81.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="http://example.com/2025/01/17/%E9%9D%A2%E8%AF%95%20hive%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%981/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Tech智汇站</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/tags/%E9%9D%A2%E8%AF%95/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>面试<span class="tagsPageCount">12</span></a></div></div></div><div class="post_share"><div class="social-share" data-image="/img/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/01/17/%E9%9D%A2%E8%AF%95%20spring%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">面试 spring常见面试题</div></div></a></div><div class="next-post pull-right"><a href="/2025/01/17/%E9%9D%A2%E8%AF%95%20hive%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%982/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">面试 hive常见面试题2</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="anzhiyufont anzhiyu-icon-thumbs-up fa-fw" style="font-size: 1.5rem; margin-right: 4px"></i><span>喜欢这篇文章的人也看了</span></div><div class="relatedPosts-list"><div><a href="/2025/01/17/%E5%B0%9A%E8%A7%82%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B9%8Boracle%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%9F%BA%E7%A1%80%E9%83%A8%E5%88%86%201/" title="尚观云计算之oracle数据库 基础部分 1"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-01-17</div><div class="title">尚观云计算之oracle数据库 基础部分 1</div></div></a></div><div><a href="/2025/01/17/%E9%9D%A2%E8%AF%95%20ES%E9%9D%A2%E8%AF%951/" title="面试 ES面试1"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-01-17</div><div class="title">面试 ES面试1</div></div></a></div><div><a href="/2025/01/17/%E9%9D%A2%E8%AF%95%20SHELL%E8%84%9A%E6%9C%AC%E5%BC%80%E5%8F%91/" title="面试 SHELL脚本开发"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-01-17</div><div class="title">面试 SHELL脚本开发</div></div></a></div><div><a href="/2025/01/17/%E9%9D%A2%E8%AF%95%20Hadoop%E9%9D%A2%E8%AF%95/" title="面试 Hadoop面试"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-01-17</div><div class="title">面试 Hadoop面试</div></div></a></div><div><a href="/2025/01/17/%E9%9D%A2%E8%AF%95%20hive%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%982/" title="面试 hive常见面试题2"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-01-17</div><div class="title">面试 hive常见面试题2</div></div></a></div><div><a href="/2025/01/17/%E9%9D%A2%E8%AF%95%20spring%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/" title="面试 spring常见面试题"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-01-17</div><div class="title">面试 spring常见面试题</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info__sayhi" id="author-info__sayhi" onclick="anzhiyu.changeSayHelloText()"></div><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-status"><img class="g-status" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/08/24/64e6ce9c507bb.png" alt="status"/></div></div><div class="author-info__description"><div style="line-height:1.38;margin:0.6rem 0;text-align:justify;color:rgba(255, 255, 255, 0.8);">这有关于<b style="color:#fff">产品、设计、开发</b>相关的问题和看法，还有<b style="color:#fff">文章翻译</b>和<b style="color:#fff">分享</b>。</div><div style="line-height:1.38;margin:0.6rem 0;text-align:justify;color:rgba(255, 255, 255, 0.8);">相信你可以在这里找到对你有用的<b style="color:#fff">知识</b>和<b style="color:#fff">教程</b>。</div></div><div class="author-info__bottom-group"><a class="author-info__bottom-group-left" href="/about"><h1 class="author-info__name">智汇君</h1><div class="author-info__desc">路漫漫其修远兮，吾将上下而求索！</div></a><div class="card-info-social-icons is-center"><a class="social-icon faa-parent animated-hover" href="/contact/" target="_blank" title="Email"><i class="anzhiyufont anzhiyu-icon-envelope"></i></a></div></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bullhorn anzhiyu-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来看我的博客鸭~</div></div><div class="card-widget anzhiyu-right-widget" id="card-wechat" onclick="null"><div id="flip-wrapper"><div id="flip-content"><div class="face" style="background: url(https://bu.dusays.com/2023/01/13/63c02edf44033.png) center center / 100% no-repeat"></div><div class="back face" style="background: url(/img/博客微信公众号图片.png) center center / 100% no-repeat"></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#hive%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%981"><span class="toc-number">1.</span> <span class="toc-text">hive常见面试题1</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive%E6%9C%89%E7%B4%A2%E5%BC%95%E5%90%97%EF%BC%9F"><span class="toc-number">1.1.</span> <span class="toc-text">Hive有索引吗？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8Map%E7%AB%AF%E5%92%8CReduce%E7%AB%AF%E8%BF%9B%E8%A1%8Cjoin%E7%9A%84%E4%B8%8D%E5%90%8C%E5%9C%BA%E6%99%AF%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">1.2.</span> <span class="toc-text">在Map端和Reduce端进行join的不同场景是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive%E7%9A%84%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F%E5%90%84%E8%87%AA%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">1.3.</span> <span class="toc-text">Hive的存储格式有哪些？各自的优缺点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96Hive%E6%9F%A5%E8%AF%A2%E6%80%A7%E8%83%BD%EF%BC%9F"><span class="toc-number">1.4.</span> <span class="toc-text">如何优化Hive查询性能？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive%E4%B8%AD%E7%9A%84Sort-By%E3%80%81Order-By%E3%80%81Cluster-By%E3%80%81Distribute-By%E5%90%84%E4%BB%A3%E8%A1%A8%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D%EF%BC%9F"><span class="toc-number">1.5.</span> <span class="toc-text">Hive中的Sort By、Order By、Cluster By、Distribute By各代表什么意思？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%BC%8F%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E5%88%86%E5%88%AB%E9%80%82%E7%94%A8%E4%BA%8E%E4%BB%80%E4%B9%88%E5%9C%BA%E6%99%AF%EF%BC%9F"><span class="toc-number">1.6.</span> <span class="toc-text">Hive的两种模式是什么？分别适用于什么场景？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E4%BB%93%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%BF%9B%E8%A1%8C%E5%88%86%E5%B1%82%EF%BC%9F"><span class="toc-number">1.7.</span> <span class="toc-text">数仓为什么要进行分层？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E4%BA%8EHive%E6%9C%89%E5%93%AA%E4%BA%9B%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BC%98%E5%8C%96%EF%BC%9F"><span class="toc-number">1.8.</span> <span class="toc-text">关于Hive有哪些常见的优化？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">1.9.</span> <span class="toc-text">如何处理数据倾斜问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive%E4%B8%AD%E6%8E%92%E5%BA%8F%E5%87%BD%E6%95%B0%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">1.10.</span> <span class="toc-text">Hive中排序函数的区别？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%88%86%E5%8C%BA%EF%BC%9F"><span class="toc-number">1.11.</span> <span class="toc-text">Hive如何实现分区？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AF%BC%E5%85%A5%E5%92%8C%E5%AF%BC%E5%87%BA%EF%BC%9F"><span class="toc-number">1.12.</span> <span class="toc-text">Hive如何进行数据的导入和导出？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0-UDF-UDTF"><span class="toc-number">1.13.</span> <span class="toc-text">自定义函数 UDF UDTF</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%95%BF%E5%BA%A6"><span class="toc-number">1.13.1.</span> <span class="toc-text">计算字符串长度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%97%E8%BD%AC%E8%A1%8C-%E5%8D%95%E5%88%97"><span class="toc-number">1.13.2.</span> <span class="toc-text">列转行 单列</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%97%E8%BD%AC%E8%A1%8C-%E5%A4%9A%E5%88%97"><span class="toc-number">1.13.3.</span> <span class="toc-text">列转行 多列</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0-UDF-%E5%AE%9E%E7%8E%B0%E5%9C%A8SQL%E4%B8%AD%E5%B0%86%E5%8D%95%E8%AF%8D%E9%A6%96%E5%AD%97%E6%AF%8D%E8%BD%AC%E6%8D%A2%E6%88%90%E5%A4%A7%E5%86%99"><span class="toc-number">1.13.4.</span> <span class="toc-text">通过自定义函数(UDF)实现在SQL中将单词首字母转换成大写</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#36%E3%80%81%E3%80%90%E5%AD%A6%E4%B9%A0%E4%BB%BB%E5%8A%A1%E3%80%91%E9%A1%B9%E7%9B%AE%E4%BB%BB%E5%8A%A1-%E4%BD%BF%E7%94%A8Hive%E5%8A%A0%E8%BD%BD%E6%8C%87%E5%AE%9A%E6%A0%BC%E5%BC%8F%E6%95%B0%E6%8D%AE"><span class="toc-number">1.13.5.</span> <span class="toc-text">36、【学习任务】项目任务-使用Hive加载指定格式数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1"><span class="toc-number">1.13.5.1.</span> <span class="toc-text">1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%B8%80%E4%B8%AA%E5%A4%8D%E6%9D%82%E7%9A%84%E6%A1%88%E4%BE%8B"><span class="toc-number">1.13.5.2.</span> <span class="toc-text">2 一个复杂的案例</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%90%E7%BB%B4%E5%A6%82%E4%BD%95%E5%AF%B9-hive-%E8%BF%9B%E8%A1%8C%E8%B0%83%E5%BA%A6"><span class="toc-number">1.14.</span> <span class="toc-text">运维如何对 hive 进行调度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E8%BF%87Hive%E8%A7%A3%E6%9E%90JSON%E4%B8%B2%E5%90%97"><span class="toc-number">1.15.</span> <span class="toc-text">使用过Hive解析JSON串吗</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#get-json-object-%E8%87%AA%E5%B8%A6"><span class="toc-number">1.15.1.</span> <span class="toc-text">get_json_object 自带</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#json-tuple-%E8%87%AA%E5%B8%A6"><span class="toc-number">1.15.2.</span> <span class="toc-text">json_tuple 自带</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive%E8%A7%A3%E6%9E%90json%E6%95%B0%E7%BB%84"><span class="toc-number">1.15.3.</span> <span class="toc-text">Hive解析json数组</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B5%8C%E5%A5%97%E5%AD%90%E6%9F%A5%E8%AF%A2%E8%A7%A3%E6%9E%90json%E6%95%B0%E7%BB%84"><span class="toc-number">1.15.3.1.</span> <span class="toc-text">嵌套子查询解析json数组</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#explode%E5%87%BD%E6%95%B0"><span class="toc-number">1.15.3.1.1.</span> <span class="toc-text">explode函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#regexp-replace%E5%87%BD%E6%95%B0"><span class="toc-number">1.15.3.1.2.</span> <span class="toc-text">regexp_replace函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#json-tuple"><span class="toc-number">1.15.3.1.3.</span> <span class="toc-text">json_tuple</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-lateral-view-%E8%A7%A3%E6%9E%90json%E6%95%B0%E7%BB%84"><span class="toc-number">1.15.3.2.</span> <span class="toc-text">使用 lateral view 解析json数组</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive-%E5%B0%8F%E6%96%87%E4%BB%B6%E8%BF%87%E5%A4%9A%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3"><span class="toc-number">1.16.</span> <span class="toc-text">Hive 小文件过多怎么解决</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E6%96%87%E4%BB%B6%E4%BA%A7%E7%94%9F%E5%8E%9F%E5%9B%A0"><span class="toc-number">1.16.1.</span> <span class="toc-text">小文件产生原因</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E6%96%87%E4%BB%B6%E8%BF%87%E5%A4%9A%E4%BA%A7%E7%94%9F%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">1.16.2.</span> <span class="toc-text">小文件过多产生的影响</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%E5%B0%8F%E6%96%87%E4%BB%B6%E8%BF%87%E5%A4%9A"><span class="toc-number">1.16.3.</span> <span class="toc-text">怎么解决小文件过多</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-hive-%E8%87%AA%E5%B8%A6%E7%9A%84-concatenate-%E5%91%BD%E4%BB%A4%EF%BC%8C%E8%87%AA%E5%8A%A8%E5%90%88%E5%B9%B6%E5%B0%8F%E6%96%87%E4%BB%B6"><span class="toc-number">1.16.3.1.</span> <span class="toc-text">使用 hive 自带的 concatenate 命令，自动合并小文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B0%83%E6%95%B4%E5%8F%82%E6%95%B0%E5%87%8F%E5%B0%91Map%E6%95%B0%E9%87%8F"><span class="toc-number">1.16.3.2.</span> <span class="toc-text">调整参数减少Map数量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%8F%E5%B0%91Reduce%E7%9A%84%E6%95%B0%E9%87%8F"><span class="toc-number">1.16.3.3.</span> <span class="toc-text">减少Reduce的数量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8hadoop%E7%9A%84archive%E5%B0%86%E5%B0%8F%E6%96%87%E4%BB%B6%E5%BD%92%E6%A1%A3"><span class="toc-number">1.16.3.4.</span> <span class="toc-text">使用hadoop的archive将小文件归档</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive-%E4%BC%98%E5%8C%96%E6%9C%89%E5%93%AA%E4%BA%9B"><span class="toc-number">1.17.</span> <span class="toc-text">Hive 优化有哪些</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E6%96%B9%E5%BC%8F"><span class="toc-number">1.17.1.</span> <span class="toc-text">Hive性能问题排查方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E8%B7%B5"><span class="toc-number">1.17.2.</span> <span class="toc-text">实践</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#join-%E8%AF%AD%E5%8F%A5%E4%BC%9A%E8%BF%87%E6%BB%A4-null-%E7%9A%84%E5%80%BC%E5%90%97%EF%BC%9F"><span class="toc-number">1.17.2.1.</span> <span class="toc-text">join 语句会过滤 null 的值吗？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#group-by-%E5%88%86%E7%BB%84%E8%AF%AD%E5%8F%A5%E4%BC%9A%E8%BF%9B%E8%A1%8C%E6%8E%92%E5%BA%8F%E5%90%97%EF%BC%9F"><span class="toc-number">1.17.2.2.</span> <span class="toc-text">group by 分组语句会进行排序吗？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%93%AA%E6%9D%A1sql%E6%89%A7%E8%A1%8C%E6%95%88%E7%8E%87%E9%AB%98%E5%91%A2%EF%BC%9F"><span class="toc-number">1.17.2.3.</span> <span class="toc-text">哪条sql执行效率高呢？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E7%9A%84%E6%96%B9%E5%BC%8F"><span class="toc-number">1.17.3.</span> <span class="toc-text">Hive性能调优的方式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-SQL%E8%AF%AD%E5%8F%A5%E4%BC%98%E5%8C%96"><span class="toc-number">1.17.3.1.</span> <span class="toc-text">1. SQL语句优化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#union-all"><span class="toc-number">1.17.3.1.1.</span> <span class="toc-text">union all</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#distinct"><span class="toc-number">1.17.3.1.2.</span> <span class="toc-text">distinct</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%E4%BC%98%E5%8C%96"><span class="toc-number">1.17.3.2.</span> <span class="toc-text">2. 数据格式优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%B0%8F%E6%96%87%E4%BB%B6%E8%BF%87%E5%A4%9A%E4%BC%98%E5%8C%96"><span class="toc-number">1.17.3.3.</span> <span class="toc-text">3. 小文件过多优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C%E4%BC%98%E5%8C%96"><span class="toc-number">1.17.3.4.</span> <span class="toc-text">4. 并行执行优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-JVM%E4%BC%98%E5%8C%96"><span class="toc-number">1.17.3.5.</span> <span class="toc-text">5. JVM优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E6%8E%A8%E6%B5%8B%E6%89%A7%E8%A1%8C%E4%BC%98%E5%8C%96"><span class="toc-number">1.17.3.6.</span> <span class="toc-text">6. 推测执行优化</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/05/14/%E5%BB%96%E9%9B%AA%E5%B3%B0PythonJava%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" title="无题"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2025/05/14/%E5%BB%96%E9%9B%AA%E5%B3%B0PythonJava%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" title="无题">无题</a><time datetime="2025-05-14T04:06:50.987Z" title="发表于 2025-05-14 12:06:50">2025-05-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/14/%E5%BB%96%E9%9B%AA%E5%B3%B0Python/" title="无题"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2025/05/14/%E5%BB%96%E9%9B%AA%E5%B3%B0Python/" title="无题">无题</a><time datetime="2025-05-14T04:02:32.996Z" title="发表于 2025-05-14 12:02:32">2025-05-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/16/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/" title="无题"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2025/04/16/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F-%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/" title="无题">无题</a><time datetime="2025-04-16T02:11:28.562Z" title="发表于 2025-04-16 10:11:28">2025-04-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/17/2022%E9%BB%91%E9%A9%AC%E6%95%B0%E6%8D%AE%E6%B9%96%E6%9E%B6%E6%9E%84%E5%BC%80%E5%8F%91Hudi-%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E7%AF%87%201/" title="2022黑马数据湖架构开发Hudi"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2022黑马数据湖架构开发Hudi"/></a><div class="content"><a class="title" href="/2025/01/17/2022%E9%BB%91%E9%A9%AC%E6%95%B0%E6%8D%AE%E6%B9%96%E6%9E%B6%E6%9E%84%E5%BC%80%E5%8F%91Hudi-%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E7%AF%87%201/" title="2022黑马数据湖架构开发Hudi">2022黑马数据湖架构开发Hudi</a><time datetime="2025-01-17T09:35:02.000Z" title="发表于 2025-01-17 17:35:02">2025-01-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/17/2022%E9%BB%91%E9%A9%AC%E6%95%B0%E6%8D%AE%E6%B9%96%E6%9E%B6%E6%9E%84%E5%BC%80%E5%8F%91Hudi-%E5%BA%94%E7%94%A8%E8%BF%9B%E9%98%B6%E7%AF%87%20flink%E9%9B%86%E6%88%90%201/" title="2022黑马数据湖架构开发Hudi-应用进阶篇 flink集成 1"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/%E6%96%87%E7%AB%A0%E5%B0%81%E9%9D%A2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2022黑马数据湖架构开发Hudi-应用进阶篇 flink集成 1"/></a><div class="content"><a class="title" href="/2025/01/17/2022%E9%BB%91%E9%A9%AC%E6%95%B0%E6%8D%AE%E6%B9%96%E6%9E%B6%E6%9E%84%E5%BC%80%E5%8F%91Hudi-%E5%BA%94%E7%94%A8%E8%BF%9B%E9%98%B6%E7%AF%87%20flink%E9%9B%86%E6%88%90%201/" title="2022黑马数据湖架构开发Hudi-应用进阶篇 flink集成 1">2022黑马数据湖架构开发Hudi-应用进阶篇 flink集成 1</a><time datetime="2025-01-17T09:35:02.000Z" title="发表于 2025-01-17 17:35:02">2025-01-17</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div id="workboard"><img class="workSituationImg boardsign" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.0.4/img/badge/安知鱼-上班摸鱼中.svg" alt="距离月入25k也就还差一个大佬带我~" title="距离月入25k也就还差一个大佬带我~"/><div id="runtimeTextTip"></div></div></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2024 - 2025 By <a class="footer-bar-link" href="/" title="智汇君" target="_blank">智汇君</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">80</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">30</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://video-b2g.pages.dev/" title="视频解析"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/视频.png" alt="视频解析"/><span class="back-menu-item-text">视频解析</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://md5-uir.pages.dev/" title="md5值解密"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/MD5.png" alt="md5值解密"/><span class="back-menu-item-text">md5值解密</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://screencheck.pages.dev/#welcome" title="屏幕坏点检查"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/屏幕外观.png" alt="屏幕坏点检查"/><span class="back-menu-item-text">屏幕坏点检查</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://htmleditor-9lo.pages.dev/" title="在线html编辑器"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/html编辑器.png" alt="在线html编辑器"/><span class="back-menu-item-text">在线html编辑器</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">博客</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://ttyong-github-io.pages.dev/" title="旧博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/博客.png" alt="旧博客"/><span class="back-menu-item-text">旧博客</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 友链</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><i class="anzhiyufont anzhiyu-icon-link faa-tada" style="font-size: 0.9em;"></i><span> 友人帐</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> 关于本人</span></a></li></ul></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/Git/" style="font-size: 0.88rem;">Git<sup>1</sup></a><a href="/tags/GitHub/" style="font-size: 0.88rem;">GitHub<sup>1</sup></a><a href="/tags/Hexo/" style="font-size: 0.88rem;">Hexo<sup>1</sup></a><a href="/tags/NodeJs/" style="font-size: 0.88rem;">NodeJs<sup>1</sup></a><a href="/tags/git/" style="font-size: 0.88rem;">git<sup>3</sup></a><a href="/tags/github/" style="font-size: 0.88rem;">github<sup>1</sup></a><a href="/tags/idea/" style="font-size: 0.88rem;">idea<sup>1</sup></a><a href="/tags/%E5%85%AC%E8%80%83%E6%9D%A8%E8%80%81%E5%B8%88/" style="font-size: 0.88rem;">公考杨老师<sup>10</sup></a><a href="/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 0.88rem;">博客<sup>1</sup></a><a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 0.88rem;">大数据<sup>26</sup></a><a href="/tags/%E5%AE%89%E7%9F%A5%E9%B1%BC%E4%B8%BB%E9%A2%98/" style="font-size: 0.88rem;">安知鱼主题<sup>1</sup></a><a href="/tags/%E5%BB%96%E9%9B%AA%E5%B3%B0/" style="font-size: 0.88rem;">廖雪峰<sup>9</sup></a><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" style="font-size: 0.88rem;">计算机网络<sup>1</sup></a><a href="/tags/%E9%9D%A2%E8%AF%95/" style="font-size: 0.88rem;">面试<sup>12</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.7"></meting-js></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("24/12/2024 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2024 By 安知鱼 V1.6.14",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 智汇君 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><script async="async">(function () {
  var grt = new Date("24/12/2024 00:00:00"); //设置网站上线时间
  var now = new Date();
  var dnum;
  var hnum;
  var mnum;
  var snum;
  var nowHour;

  // 计算并更新天数、小时数、分钟数和秒数
  function updateTime() {
    now = new Date(); // 更新 now 的值
    nowHour = now.getHours(); // 更新 nowHour 的值
    var days = (now - grt) / 1000 / 60 / 60 / 24;
    dnum = Math.floor(days);
    var hours = (now - grt) / 1000 / 60 / 60 - 24 * dnum;
    hnum = Math.floor(hours);
    if (String(hnum).length == 1) {
      hnum = "0" + hnum;
    }
    var minutes = (now - grt) / 1000 / 60 - 24 * 60 * dnum - 60 * hnum;
    mnum = Math.floor(minutes);
    if (String(mnum).length == 1) {
      mnum = "0" + mnum;
    }
    var seconds = (now - grt) / 1000 - 24 * 60 * 60 * dnum - 60 * 60 * hnum - 60 * mnum;
    snum = Math.round(seconds);
    if (String(snum).length == 1) {
      snum = "0" + snum;
    }
  }

  // 更新网页中显示的网站运行时间
  function updateHtml() {
    const footer = document.getElementById("footer");
    if (!footer) return
    let currentTimeHtml = "";
    if (nowHour < 18 && nowHour >= 9) {
      // 如果是上班时间，默认就是"安知鱼-上班摸鱼中.svg"图片，不需要更改
      currentTimeHtml = `本站居然运行了 ${dnum} 天<span id='runtime'> ${hnum} 小时 ${mnum} 分 ${snum} 秒 </span><i class='anzhiyufont anzhiyu-icon-heartbeat' style='color:red'></i>`;
    } else {
      // 如果是下班时间，插入"安知鱼-下班啦.svg"图片
      let img = document.querySelector("#workboard .workSituationImg");
      if (img != null) {
        img.src = "https://npm.elemecdn.com/anzhiyu-blog@2.0.4/img/badge/安知鱼-下班啦.svg";
        img.title = "下班了就该开开心心的玩耍，嘿嘿~";
        img.alt = "下班了就该开开心心的玩耍，嘿嘿~";
      }

      currentTimeHtml = `本站居然运行了 ${dnum} 天<span id='runtime'> ${hnum} 小时 ${mnum} 分 ${snum} 秒 </span><i class='anzhiyufont anzhiyu-icon-heartbeat' style='color:red'></i>`;
    }

    if (document.getElementById("runtimeTextTip")) {
      document.getElementById("runtimeTextTip").innerHTML = currentTimeHtml;
    }
  }

  setInterval(() => {
    updateTime();
    updateHtml();
  }, 1000);
})();</script><div class="js-pjax"><input type="hidden" name="page-type" id="page-type" value="post"></div><script async data-pjax src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.1/bubble/bubble.js"></script><script>var visitorMail = "";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><script src="/js/anzhiyu/right_click_menu.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><script async src="https://at.alicdn.com/t/c/font_4792307_02gxbpyxcpwp.js?spm=a313x.manage_type_myprojects.i1.10.56323a81KZGFDE&amp;file=font_4792307_02gxbpyxcpwp.js# 阿里图标symbol 引用链接，主题会进行加载 symbol 引用"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>